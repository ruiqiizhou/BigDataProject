{"cells": [{"cell_type": "markdown", "id": "d7cd5e03-4797-45bd-9180-84bf39e660f4", "metadata": {}, "source": "# Milestone 4"}, {"cell_type": "code", "execution_count": 1, "id": "09fef663-5bee-48a4-ba0c-04f96a0a8159", "metadata": {}, "outputs": [], "source": "import pandas as pd"}, {"cell_type": "code", "execution_count": 2, "id": "268dfcbe-361e-471f-aa1e-8522d6ab8b2f", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession, Row\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier, LinearSVC, OneVsRest\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator"}, {"cell_type": "code", "execution_count": null, "id": "c8b207ea-3f4f-4efc-8a72-0beddbc7804b", "metadata": {}, "outputs": [], "source": "#!pip install h2o"}, {"cell_type": "code", "execution_count": 4, "id": "5759e000-785f-48e8-9e2b-2abd55ab2dbd", "metadata": {}, "outputs": [], "source": "# import h2o\n# from h2o.automl import H2OAutoML"}, {"cell_type": "code", "execution_count": 3, "id": "60743741-11a5-4862-9c36-099473be18ac", "metadata": {}, "outputs": [], "source": "###### Define train-test split ######\ntrain_split = 0.8\ntest_split = 0.2\n\n###### Define target label ######\ntarget_column = 'Delay'"}, {"cell_type": "code", "execution_count": 4, "id": "dc108858-50be-4ca9-b364-a679e792b858", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/21 10:18:26 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n24/04/21 10:18:26 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n24/04/21 10:18:26 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n24/04/21 10:18:26 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n24/04/21 10:18:36 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n                                                                                \r"}], "source": "###\n#for whole dataset\n##### Set up session & data ######\n#Initialize Spark session\nspark = SparkSession.builder.appName(\"DecisionTreeJob\").getOrCreate()\n\n#Import the data as an RDD\n#csv_rdd = spark.sparkContext.textFile(\"gs://final-bucket-jy/balanced_flight_encoded.csv/part-00197-1a134fbf-5193-4064-89a5-cb01a8fb1d12-c000.csv\")\ncsv_rdd = spark.sparkContext.textFile(\"gs://final-bucket-jy/balanced_flight_encoded.csv\")\n\n\n#Convert the RDD to a DataFrame\ncsv_rows = csv_rdd.map(lambda line: line.split(\",\"))\nheader = csv_rows.first()  # Extract header\ncsv_data = csv_rows.filter(lambda row: row != header).map(lambda row: Row(**{header[i]: float(row[i]) for i in range(len(header))}))"}, {"cell_type": "code", "execution_count": 4, "id": "f67dc83c-50a0-4fe1-bdb6-ed75b84e97b8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/21 10:28:18 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n24/04/21 10:28:18 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n24/04/21 10:28:18 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n24/04/21 10:28:18 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n24/04/21 10:28:28 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \njava.lang.InterruptedException\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n                                                                                \r"}], "source": "######\n# for sample data\n######\nspark = SparkSession.builder.appName(\"DecisionTreeJob\").getOrCreate()\n\ncsv_rdd1 = spark.sparkContext.textFile(\"gs://final-bucket-jy/balanced_flight_encoded.csv/part-00121-ef9393a8-06b1-4cd8-8720-6fb2f864254a-c000.csv\")\ncsv_rdd2 = spark.sparkContext.textFile(\"gs://final-bucket-jy/balanced_flight_encoded.csv/part-00321-ef9393a8-06b1-4cd8-8720-6fb2f864254a-c000.csv\")\n\n# Convert the RDD1 to a DataFrame\ncsv_rows1 = csv_rdd1.map(lambda line: line.split(\",\"))\nheader = csv_rows1.first()  # Extract header\ncsv_data1 = csv_rows1.filter(lambda row: row != header).map(lambda row: Row(**{header[i]: float(row[i]) for i in range(len(header))}))\n\n# Convert the RDD2 to a DataFrame\ncsv_rows2 = csv_rdd2.map(lambda line: line.split(\",\"))\ncsv_data2 = csv_rows2.filter(lambda row: row != header).map(lambda row: Row(**{header[i]: float(row[i]) for i in range(len(header))}))"}, {"cell_type": "code", "execution_count": 5, "id": "f25d9d76-2729-4e6d-9f71-04d049a218b4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Create a DataFrame from the RDD of Rows\nsampled_df = spark.createDataFrame(csv_data)\n\n# Limit the number of rows to 100\n#sampled_df = sampled_df.limit(50)"}, {"cell_type": "code", "execution_count": 38, "id": "c8b27a53-e6fb-4f83-9c64-338d4eda163f", "metadata": {}, "outputs": [], "source": "sampled_df1 = spark.createDataFrame(csv_data1)\nsampled_df2 = spark.createDataFrame(csv_data2)"}, {"cell_type": "code", "execution_count": 39, "id": "8dfee181-4c88-42c1-9efa-1558ee092640", "metadata": {}, "outputs": [], "source": "# # Limit the number of rows to 100\n# sampled_df1 = sampled_df1.limit(20000)\n# sampled_df2 = sampled_df2.limit(20000)\nsampled_df = sampled_df1.union(sampled_df2)\nfeature_df = sampled_df.drop('Delay')"}, {"cell_type": "code", "execution_count": 7, "id": "1ede6f13-c583-4fa7-bfd5-3eac49e1075b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/15 05:03:57 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}, {"data": {"text/plain": "53316"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "sampled_df.count()"}, {"cell_type": "code", "execution_count": 6, "id": "4f8ec9f1-afce-4f51-a935-07d083c0c5a5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 09:34:07 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  0.0|26657|\n|  1.0|26659|\n+-----+-----+\n\n"}], "source": "counts = sampled_df.groupBy('Delay').count()\ncounts.show()"}, {"cell_type": "code", "execution_count": 16, "id": "34a7c661-ee2d-4cd9-9335-44a348f35ed1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  0.0|  500|\n+-----+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "counts1 = sampled_df1.groupBy('Delay').count()\ncounts1.show()"}, {"cell_type": "code", "execution_count": 17, "id": "8b6740ce-dca9-40c7-a13d-ad14184a2d67", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  1.0|  500|\n+-----+-----+\n\n"}], "source": "counts2 = sampled_df2.groupBy('Delay').count()\ncounts2.show()"}, {"cell_type": "code", "execution_count": 21, "id": "7e1aed50-5999-4dfa-b2fb-1fb71eba503d", "metadata": {}, "outputs": [], "source": "sampled_df = sampled_df1.union(sampled_df2)"}, {"cell_type": "code", "execution_count": 24, "id": "4400beb8-8a76-424d-af6b-e8963f27fb22", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  0.0|   60|\n|  1.0|   60|\n+-----+-----+\n\n"}], "source": "counts = sampled_df.groupBy('Delay').count()\ncounts.show()"}, {"cell_type": "code", "execution_count": 13, "id": "2d5137d2-29a1-48f7-8b14-30bf06b56f30", "metadata": {}, "outputs": [], "source": "# # Import the data as an RDD\n# csv_rdd1 = spark.sparkContext.textFile(\"gs://final-bucket-jy/balanced_flight_encoded.csv/part-00285-1a134fbf-5193-4064-89a5-cb01a8fb1d12-c000.csv\")\n\n# csv_rows1 = csv_rdd1.map(lambda line: line.split(\",\"))\n# header = csv_rows1.first()  # Extract header\n# csv_data1 = csv_rows1.filter(lambda row: row != header).map(lambda row: Row(**{header[i]: float(row[i]) for i in range(len(header))}))"}, {"cell_type": "code", "execution_count": 14, "id": "57cb1180-0129-4488-aa9e-9b6872f447f2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  0.0| 1298|\n+-----+-----+\n\n"}], "source": "# # Create a DataFrame from the RDD of Rows\n# sampled_df1 = spark.createDataFrame(csv_data)\n\n# counts1 = sampled_df1.groupBy('Delay').count()\n# counts1.show()"}, {"cell_type": "code", "execution_count": 18, "id": "440b292d-d7cb-4b7b-9fc6-385431fd104e", "metadata": {}, "outputs": [{"data": {"text/plain": "27"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "sampled_df.columns.index('Delay')"}, {"cell_type": "code", "execution_count": 22, "id": "c62568f3-6f07-443c-aa7d-947a2d6dfc67", "metadata": {}, "outputs": [], "source": "feature_df = sampled_df.drop('Delay')"}, {"cell_type": "code", "execution_count": 8, "id": "c1fa8e03-b726-4531-82f0-38b4bc5fcab6", "metadata": {}, "outputs": [{"ename": "ValueError", "evalue": "'Delay' is not in list", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDelay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n", "\u001b[0;31mValueError\u001b[0m: 'Delay' is not in list"]}], "source": "feature_df.columns.index('Delay')"}, {"cell_type": "code", "execution_count": 9, "id": "9f769867-3eb0-4fff-a85a-9dc06de218f3", "metadata": {}, "outputs": [{"data": {"text/plain": "112"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "len(sampled_df.columns)"}, {"cell_type": "markdown", "id": "19d4c77a-d22b-48fc-8225-36b39f0af62d", "metadata": {}, "source": "# Modeling"}, {"cell_type": "code", "execution_count": 40, "id": "696094b7-e87b-4f13-ad19-b3be4ea53560", "metadata": {}, "outputs": [], "source": "# Define feature columns\nfeature_columns = feature_df.columns  # Assuming the last column is the target\n\n# Assemble features into a single vector column\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n#sampled_df_model = assembler.transform(sampled_df)\n# Define a simple pipeline\npipeline = Pipeline(stages=[assembler])\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(sampled_df)\n\n# Transform the data with the pipeline\nsampled_df_transformed = pipelineModel.transform(sampled_df)\n# Select features and target column for modeling\nsampled_df_model = sampled_df_transformed.select(\"features\", target_column)\n\n# Split data into training and test sets\ntrain_df, test_df = sampled_df_model.randomSplit([train_split, test_split])"}, {"cell_type": "code", "execution_count": 9, "id": "e72949f1-d5a8-4882-b8b4-9202294db0ee", "metadata": {}, "outputs": [], "source": "###### Record accuracy ######\nrecords = {} # the item we will collect \"model_name\": train_accuracy, train_auc, test_accuracy, test_auc"}, {"cell_type": "code", "execution_count": 72, "id": "1fbc571a-38e1-4980-b106-cec5f86df7b7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  0.0| 1145|\n|  1.0| 1611|\n+-----+-----+\n\n"}], "source": "c = train_df.groupBy('Delay').count()\nc.show()"}, {"cell_type": "code", "execution_count": 73, "id": "acb51011-a078-45af-b5de-9b793db170d5", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|Delay|count|\n+-----+-----+\n|  0.0|  264|\n|  1.0|  396|\n+-----+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "c = test_df.groupBy('Delay').count()\nc.show()"}, {"cell_type": "markdown", "id": "87bf5c22-9bc8-48a2-bf50-006c6b71c737", "metadata": {}, "source": "## 1. Logistic Regression"}, {"cell_type": "code", "execution_count": 43, "id": "a3bd0842-62f4-4010-bd4b-eb5ff3f69855", "metadata": {}, "outputs": [], "source": "# Train a logistic regression model on the subset of data\nlr = LogisticRegression(featuresCol='features', labelCol=target_column)\n#lr_Model = lr.fit(train_df)"}, {"cell_type": "code", "execution_count": 26, "id": "25759649-54d1-4aa4-9d76-ac2e472a960a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.7947810184751009\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.866910050403447\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Test Accuracy: 0.7871342835708928\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8597017031080224\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = lr_Model.transform(train_df)\n# by accuracy\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\n\n# by auc\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\")\nevaluator_auc.setMetricName(\"areaUnderROC\")\n\naccuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {accuracy}\")\n\nauc = evaluator_auc.evaluate(train_predictions, {evaluator_auc.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc}\")\n\n# Evaluate the model on the test set by accuracy\npredictions = lr_Model.transform(test_df)\n# by accuracy\naccuracy_test = evaluator_acc.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_test = evaluator_auc.evaluate(predictions, {evaluator_auc.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 27, "id": "849553f0-3eb0-4b67-9569-c9e504de1dea", "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'records' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecords\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic regression\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [accuracy, auc, accuracy_test, auc_test]\n", "\u001b[0;31mNameError\u001b[0m: name 'records' is not defined"]}], "source": "records['logistic regression'] = [accuracy, auc, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "da5e37c1-84f3-4db7-8a68-1f66cffeb661", "metadata": {}, "source": "## 2. Decision Tree"}, {"cell_type": "code", "execution_count": 49, "id": "eac0d4cc-5f93-4f8d-9b40-b3fda4335d2c", "metadata": {}, "outputs": [], "source": "###### Modeling ######\n# Initialize the DecisionTreeClassifier\ndt = DecisionTreeClassifier(labelCol=target_column, featuresCol=\"features\")\n\n# Train the decision tree model\n#dt_model = dt.fit(train_df)"}, {"cell_type": "code", "execution_count": 29, "id": "226890cc-4835-45b8-993b-154ad9cc26dc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.7772202944762262\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8120758443534942\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 317:===================================================> (165 + 4) / 169]\r"}, {"name": "stdout", "output_type": "stream", "text": "Test Accuracy: 0.77344336084021\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8069675488377694\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = dt_model.transform(train_df)\n\n# by accuracy\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\n\n# by auc\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\")\nevaluator_auc.setMetricName(\"areaUnderROC\")\n\naccuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {accuracy}\")\nauc = evaluator_auc.evaluate(train_predictions, {evaluator_auc.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc}\")\n\n# Evaluate the model on the test set by accuracy\npredictions = dt_model.transform(test_df)\n# by accuracy\naccuracy_test = evaluator_acc.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_test = evaluator_auc.evaluate(predictions, {evaluator_auc.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 21, "id": "3f5a97a8-ac98-4ed8-bfd2-4495d3ac9a6c", "metadata": {}, "outputs": [], "source": "records['decision tree'] = [accuracy, auc, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "611a41c4-9958-4804-82a4-12e3daca8336", "metadata": {}, "source": "## 3. Random Forest Classifier"}, {"cell_type": "code", "execution_count": 54, "id": "20a56fb8-7cad-4a5b-adf5-f0383bd5f12d", "metadata": {}, "outputs": [], "source": "###### Modeling ######\n# Initialize the RandomForestClassifier\nrf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\")\n\n# Train the decision tree model\n#rf_model = rf.fit(train_df)"}, {"cell_type": "code", "execution_count": 31, "id": "c8910b5e-ce78-44c2-9304-7a64e174d0a9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.7931867204351496\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8590832097730543\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Test Accuracy: 0.7823518379594899\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8475931350322239\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = rf_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\ntrain_auc = evaluator_auc.evaluate(train_predictions)\nprint(f\"Area Under ROC: {train_auc}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = rf_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\n\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": 24, "id": "f090efa7-ad26-4a20-a387-efd1756260f2", "metadata": {}, "outputs": [], "source": "records['random forest classifier'] = [train_accuracy, train_auc, test_accuracy, auc]"}, {"cell_type": "markdown", "id": "753a8361-aee6-4d40-ab40-4f8dd93aa436", "metadata": {}, "source": "## 4. Gradient Boosted Decision Trees"}, {"cell_type": "code", "execution_count": 63, "id": "8fdd98c2-0271-4cb0-88ed-6f9f4ee190fc", "metadata": {}, "outputs": [], "source": "# Initialize Gradient Boosted Decision Trees Classifier\ngbt = GBTClassifier(labelCol=target_column, featuresCol=\"features\")\n\n# Train the model\n#gbt_model = gbt.fit(train_df)"}, {"cell_type": "code", "execution_count": 33, "id": "0075cdfe-515d-483c-9dad-3df8d139bd8b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 02:18:12 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1712542704209_0040_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:18:12.517]Container killed on request. Exit code is 143\n[2024-04-21 02:18:12.517]Container exited with a non-zero exit code 143. \n[2024-04-21 02:18:12.519]Killed by external signal\n.\n24/04/21 02:18:12 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 5 on final-cluster-jy-m.us-central1-c.c.final-project-419701.internal: Container from a bad node: container_1712542704209_0040_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:18:12.517]Container killed on request. Exit code is 143\n[2024-04-21 02:18:12.517]Container exited with a non-zero exit code 143. \n[2024-04-21 02:18:12.519]Killed by external signal\n.\n24/04/21 02:18:12 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 116.0 in stage 579.0 (TID 69345) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0040_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:18:12.517]Container killed on request. Exit code is 143\n[2024-04-21 02:18:12.517]Container exited with a non-zero exit code 143. \n[2024-04-21 02:18:12.519]Killed by external signal\n.\n24/04/21 02:18:12 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 113.0 in stage 579.0 (TID 69342) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0040_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:18:12.517]Container killed on request. Exit code is 143\n[2024-04-21 02:18:12.517]Container exited with a non-zero exit code 143. \n[2024-04-21 02:18:12.519]Killed by external signal\n.\n24/04/21 02:18:12 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1712542704209_0040_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:18:12.517]Container killed on request. Exit code is 143\n[2024-04-21 02:18:12.517]Container exited with a non-zero exit code 143. \n[2024-04-21 02:18:12.519]Killed by external signal\n.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.8662196380005627\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.9529277662583844\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Test Accuracy: 0.8555888972243061\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.9480334625867375\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = gbt_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\ntrain_auc = evaluator_auc.evaluate(train_predictions)\nprint(f\"Area Under ROC: {train_auc}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = gbt_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\n\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": 11, "id": "35d303ad-6cf4-4d01-8d17-b9bb95b18d13", "metadata": {}, "outputs": [], "source": "records['gradient boosted decision tree'] = [train_accuracy, train_auc, test_accuracy, auc]"}, {"cell_type": "markdown", "id": "02d30937-2b5b-462d-bdf0-98644b7c63be", "metadata": {}, "source": "## 5. (Run on smaller data) Multilayer perceptron (MLP) "}, {"cell_type": "code", "execution_count": 11, "id": "a4a97b96-5b96-4826-b638-daa897873376", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 10:13:26 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n24/04/21 10:13:26 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Test set accuracy = 0.5116279069767442\nTest Accuracy: 0.7142857142857143\nArea Under ROC: 0.75\n"}], "source": "layers = [111, 64, 32, 2]\n\nmlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, labelCol=target_column, featuresCol=\"features\")\n\nmlp_model = mlp.fit(train_df)\nresult = mlp_model.transform(train_df)#mlp_model.transform(test_df)\n\nevaluator = MulticlassClassificationEvaluator(labelCol=target_column,metricName=\"accuracy\")\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n\naccuracy = evaluator.evaluate(result)\nprint(\"Test set accuracy = \" + str(accuracy))\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = mlp_model.transform(test_df)\ntest_accuracy = evaluator.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\n\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": 12, "id": "53ed0a51-cde4-4638-9c74-ecc19a3b8ef7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 05:37:38 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n24/04/21 05:37:38 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n                                                                                \r"}], "source": "# define layers for the neural network\n# [input layer of size 114, two intermdeidate of size 64 and 32, output size 2]\nlayers = [10, 2,2]\n\n# Create the trainer and set its parameters\nmlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, labelCol=target_column, featuresCol=\"features\")\n\n# Train the model\nmlp_model = mlp.fit(train_df)"}, {"cell_type": "code", "execution_count": 41, "id": "0b2a0ac3-846c-4d4d-be71-49ac52bd5876", "metadata": {}, "outputs": [], "source": "# define layers for the neural network\n# [input layer of size 114, two intermdeidate of size 64 and 32, output size 2]\nlayers = [111, 64, 32, 2]\n\n# Create the trainer and set its parameters\nmlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, labelCol=target_column, featuresCol=\"features\")\n\n# Train the model\nmlp_model = mlp.fit(train_df)"}, {"cell_type": "code", "execution_count": 42, "id": "c7eb8297-d823-4e34-a72e-f7beaeaa3b90", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.5849056603773585\nArea Under ROC: 0.5022557797240044\nTest Accuracy: 0.6\nArea Under ROC: 0.5031756963575145\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = mlp_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\ntrain_auc = evaluator_auc.evaluate(train_predictions)\nprint(f\"Area Under ROC: {train_auc}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = mlp_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\n\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": null, "id": "c1c0a9be-987f-4528-8462-1dbed0ff540c", "metadata": {}, "outputs": [], "source": "records['multilayer perceptron'] = [train_accuracy, train_auc, test_accuracy, auc]"}, {"cell_type": "markdown", "id": "1ddfd376-9ed2-4142-a077-678f1026c490", "metadata": {}, "source": "## 6. Linear Support Vector Machine"}, {"cell_type": "code", "execution_count": 74, "id": "b5c7d57c-9242-4745-9488-3ed21e2ddb11", "metadata": {}, "outputs": [], "source": "# Initialize Linear Support Vector Machine\nlsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol=target_column, featuresCol=\"features\")\n\n# Train the model\n#lsvc_model = lsvc.fit(train_df)"}, {"cell_type": "code", "execution_count": 12, "id": "a5ca61df-3e62-4f3a-8235-d7cd2e493a12", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.7547522301620663\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.831187526181334\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Test Accuracy: 0.752334667429007\n"}, {"name": "stderr", "output_type": "stream", "text": "24/04/21 02:29:27 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1712542704209_0041_01_000002 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:29:27.205]Container killed on request. Exit code is 143\n[2024-04-21 02:29:27.205]Container exited with a non-zero exit code 143. \n[2024-04-21 02:29:27.205]Killed by external signal\n.\n24/04/21 02:29:27 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1712542704209_0041_01_000002 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:29:27.205]Container killed on request. Exit code is 143\n[2024-04-21 02:29:27.205]Container exited with a non-zero exit code 143. \n[2024-04-21 02:29:27.205]Killed by external signal\n.\n24/04/21 02:29:27 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on final-cluster-jy-m.us-central1-c.c.final-project-419701.internal: Container from a bad node: container_1712542704209_0041_01_000002 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:29:27.205]Container killed on request. Exit code is 143\n[2024-04-21 02:29:27.205]Container exited with a non-zero exit code 143. \n[2024-04-21 02:29:27.205]Killed by external signal\n.\n24/04/21 02:29:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 129.0 in stage 150.0 (TID 13837) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0041_01_000002 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:29:27.205]Container killed on request. Exit code is 143\n[2024-04-21 02:29:27.205]Container exited with a non-zero exit code 143. \n[2024-04-21 02:29:27.205]Killed by external signal\n.\n24/04/21 02:29:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 128.0 in stage 150.0 (TID 13836) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0041_01_000002 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:29:27.205]Container killed on request. Exit code is 143\n[2024-04-21 02:29:27.205]Container exited with a non-zero exit code 143. \n[2024-04-21 02:29:27.205]Killed by external signal\n.\n[Stage 160:======================================>              (123 + 2) / 169]\r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8268426801325668\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = lsvc_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\ntrain_auc = evaluator_auc.evaluate(train_predictions)\nprint(f\"Area Under ROC: {train_auc}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = lsvc_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\n\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": 15, "id": "f3b5aa6e-1f66-4b4a-a9f5-16daf533b33f", "metadata": {}, "outputs": [], "source": "records['linear support vector machine'] = [train_accuracy, train_auc, test_accuracy, auc]"}, {"cell_type": "markdown", "id": "3c80dfc1-7eca-432c-b25a-ebc83b082812", "metadata": {}, "source": "## 7. One-vs-Rest (OneVsRest)\n\nis a strategy commonly used in multi-class classification problems. In this strategy, you train multiple binary classifiers, each specializing in distinguishing one class from all other classes."}, {"cell_type": "code", "execution_count": 68, "id": "e1c2ddb3-538f-4e06-879d-6d09c390a7e7", "metadata": {}, "outputs": [], "source": "# Initialize one-vs-rest model\novr = OneVsRest(classifier=lr, labelCol=target_column, featuresCol=\"features\")\n\n# Train the model\n#ovr_model = ovr.fit(train_df)"}, {"cell_type": "code", "execution_count": 16, "id": "3262cbde-d136-4526-8ff3-e9df45807ebc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 02:34:51 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1712542704209_0041_01_000004 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:34:51.153]Container killed on request. Exit code is 143\n[2024-04-21 02:34:51.153]Container exited with a non-zero exit code 143. \n[2024-04-21 02:34:51.153]Killed by external signal\n.\n24/04/21 02:34:51 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1712542704209_0041_01_000004 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:34:51.153]Container killed on request. Exit code is 143\n[2024-04-21 02:34:51.153]Container exited with a non-zero exit code 143. \n[2024-04-21 02:34:51.153]Killed by external signal\n.\n24/04/21 02:34:51 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 4 on final-cluster-jy-m.us-central1-c.c.final-project-419701.internal: Container from a bad node: container_1712542704209_0041_01_000004 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:34:51.153]Container killed on request. Exit code is 143\n[2024-04-21 02:34:51.153]Container exited with a non-zero exit code 143. \n[2024-04-21 02:34:51.153]Killed by external signal\n.\n24/04/21 02:34:51 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 119.0 in stage 890.0 (TID 81083) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0041_01_000004 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:34:51.153]Container killed on request. Exit code is 143\n[2024-04-21 02:34:51.153]Container exited with a non-zero exit code 143. \n[2024-04-21 02:34:51.153]Killed by external signal\n.\n24/04/21 02:34:51 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 116.0 in stage 890.0 (TID 81080) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0041_01_000004 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:34:51.153]Container killed on request. Exit code is 143\n[2024-04-21 02:34:51.153]Container exited with a non-zero exit code 143. \n[2024-04-21 02:34:51.153]Killed by external signal\n.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.796179533884452\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8669025243425479\n"}, {"name": "stderr", "output_type": "stream", "text": "24/04/21 02:35:38 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1712542704209_0041_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:35:38.173]Container killed on request. Exit code is 143\n[2024-04-21 02:35:38.174]Container exited with a non-zero exit code 143. \n[2024-04-21 02:35:38.175]Killed by external signal\n.\n24/04/21 02:35:38 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1712542704209_0041_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:35:38.173]Container killed on request. Exit code is 143\n[2024-04-21 02:35:38.174]Container exited with a non-zero exit code 143. \n[2024-04-21 02:35:38.175]Killed by external signal\n.\n24/04/21 02:35:38 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 5 on final-cluster-jy-m.us-central1-c.c.final-project-419701.internal: Container from a bad node: container_1712542704209_0041_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:35:38.173]Container killed on request. Exit code is 143\n[2024-04-21 02:35:38.174]Container exited with a non-zero exit code 143. \n[2024-04-21 02:35:38.175]Killed by external signal\n.\n24/04/21 02:35:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 75.0 in stage 903.0 (TID 82393) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0041_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:35:38.173]Container killed on request. Exit code is 143\n[2024-04-21 02:35:38.174]Container exited with a non-zero exit code 143. \n[2024-04-21 02:35:38.175]Killed by external signal\n.\n24/04/21 02:35:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 77.0 in stage 903.0 (TID 82395) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1712542704209_0041_01_000005 on host: final-cluster-jy-m.us-central1-c.c.final-project-419701.internal. Exit status: 143. Diagnostics: [2024-04-21 02:35:38.173]Container killed on request. Exit code is 143\n[2024-04-21 02:35:38.174]Container exited with a non-zero exit code 143. \n[2024-04-21 02:35:38.175]Killed by external signal\n.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Test Accuracy: 0.7887364208118925\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 915:=============================================>       (144 + 5) / 169]\r"}, {"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.8597131231502189\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = ovr_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\ntrain_auc = evaluator_auc.evaluate(train_predictions)\nprint(f\"Area Under ROC: {train_auc}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = ovr_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\n\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": 23, "id": "fdb83a44-5404-42b6-930c-eddd005f6332", "metadata": {}, "outputs": [], "source": "records['linear support vector machine'] = [train_accuracy, train_auc, test_accuracy, auc]"}, {"cell_type": "markdown", "id": "4493c2a6-cea4-4900-ba66-68336ebe704d", "metadata": {}, "source": "## 8. AutoML"}, {"cell_type": "code", "execution_count": 20, "id": "1eed487a-cc16-4a06-b573-7572e9e80d17", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\nAttempting to start a local H2O server...\n  Java Version: openjdk version \"1.8.0_402\"; OpenJDK Runtime Environment (Temurin)(build 1.8.0_402-b06); OpenJDK 64-Bit Server VM (Temurin)(build 25.402-b06, mixed mode)\n  Starting server from /opt/conda/miniconda3/lib/python3.8/site-packages/h2o/backend/bin/h2o.jar\n  Ice root: /tmp/tmp5nizx8fe\n  JVM stdout: /tmp/tmp5nizx8fe/h2o_root_started_from_python.out\n  JVM stderr: /tmp/tmp5nizx8fe/h2o_root_started_from_python.err\n  Server is running at http://127.0.0.1:54321\nConnecting to H2O server at http://127.0.0.1:54321 ... successful.\n"}, {"data": {"text/html": "\n<style>\n\n#h2o-table-1.h2o-container {\n  overflow-x: auto;\n}\n#h2o-table-1 .h2o-table {\n  /* width: 100%; */\n  margin-top: 1em;\n  margin-bottom: 1em;\n}\n#h2o-table-1 .h2o-table caption {\n  white-space: nowrap;\n  caption-side: top;\n  text-align: left;\n  /* margin-left: 1em; */\n  margin: 0;\n  font-size: larger;\n}\n#h2o-table-1 .h2o-table thead {\n  white-space: nowrap; \n  position: sticky;\n  top: 0;\n  box-shadow: 0 -1px inset;\n}\n#h2o-table-1 .h2o-table tbody {\n  overflow: auto;\n}\n#h2o-table-1 .h2o-table th,\n#h2o-table-1 .h2o-table td {\n  text-align: right;\n  /* border: 1px solid; */\n}\n#h2o-table-1 .h2o-table tr:nth-child(even) {\n  /* background: #F5F5F5 */\n}\n\n</style>      \n<div id=\"h2o-table-1\" class=\"h2o-container\">\n  <table class=\"h2o-table\">\n    <caption></caption>\n    <thead></thead>\n    <tbody><tr><td>H2O_cluster_uptime:</td>\n<td>02 secs</td></tr>\n<tr><td>H2O_cluster_timezone:</td>\n<td>Etc/UTC</td></tr>\n<tr><td>H2O_data_parsing_timezone:</td>\n<td>UTC</td></tr>\n<tr><td>H2O_cluster_version:</td>\n<td>3.46.0.1</td></tr>\n<tr><td>H2O_cluster_version_age:</td>\n<td>1 month and 7 days</td></tr>\n<tr><td>H2O_cluster_name:</td>\n<td>H2O_from_python_root_tcvc8m</td></tr>\n<tr><td>H2O_cluster_total_nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O_cluster_free_memory:</td>\n<td>3.474 Gb</td></tr>\n<tr><td>H2O_cluster_total_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_allowed_cores:</td>\n<td>4</td></tr>\n<tr><td>H2O_cluster_status:</td>\n<td>locked, healthy</td></tr>\n<tr><td>H2O_connection_url:</td>\n<td>http://127.0.0.1:54321</td></tr>\n<tr><td>H2O_connection_proxy:</td>\n<td>{\"http\": null, \"https\": null}</td></tr>\n<tr><td>H2O_internal_security:</td>\n<td>False</td></tr>\n<tr><td>Python_version:</td>\n<td>3.8.15 final</td></tr></tbody>\n  </table>\n</div>\n", "text/plain": "--------------------------  -----------------------------\nH2O_cluster_uptime:         02 secs\nH2O_cluster_timezone:       Etc/UTC\nH2O_data_parsing_timezone:  UTC\nH2O_cluster_version:        3.46.0.1\nH2O_cluster_version_age:    1 month and 7 days\nH2O_cluster_name:           H2O_from_python_root_tcvc8m\nH2O_cluster_total_nodes:    1\nH2O_cluster_free_memory:    3.474 Gb\nH2O_cluster_total_cores:    4\nH2O_cluster_allowed_cores:  4\nH2O_cluster_status:         locked, healthy\nH2O_connection_url:         http://127.0.0.1:54321\nH2O_connection_proxy:       {\"http\": null, \"https\": null}\nH2O_internal_security:      False\nPython_version:             3.8.15 final\n--------------------------  -----------------------------"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Parse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\n"}], "source": "pandas_df = sampled_df.toPandas()\n\nh2o.init()\n\nh2o_df = h2o.H2OFrame(pandas_df)\n\n# Split the data into train and test sets\ntrain, test = h2o_df.split_frame(ratios=[.8], seed=1234)"}, {"cell_type": "code", "execution_count": 23, "id": "cc6ce4fd-d3b1-49fc-9979-f0934fd9a483", "metadata": {}, "outputs": [], "source": "# Identify predictors and response\nx = train.columns\ny = \"Delay\"\nx.remove(y)"}, {"cell_type": "code", "execution_count": null, "id": "773d94e9-1a6b-46ac-bb55-525ae6e495ec", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "AutoML progress: |\n02:40:10.189: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\u2588\n02:44:16.194: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\n02:44:58.556: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n02:51:02.400: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\n02:54:35.785: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\n02:55:57.537: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\n03:01:33.930: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\u2588\n03:06:48.319: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\n03:10:47.857: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\u2588\n03:16:35.814: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\n03:17:04.405: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n03:23:52.963: _train param, Dropping bad and constant columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\n03:24:18.131: _train param, Dropping unused columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\n03:24:46.334: _train param, Dropping unused columns: [arrival_airport_min_altitude, arrival_airport_min_knots, bit_0, bit_1, departure_airport_avg_altitude, departure_airport_max_altitude, arrival_airport_max_altitude, departure_airport_max_bearing, bit_4, arrival_airport_max_bearing, bit_2, bit_3, departure_airport_min_altitude, arrival_airport_avg_altitude, departure_airport_min_knots]\n\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\nmodel_id                                                     auc    logloss     aucpr    mean_per_class_error       rmse         mse\nGBM_1_AutoML_1_20240421_24010                           0.998168  0.035906   0.998257              0.00965528  0.0899433  0.0080898\nStackedEnsemble_BestOfFamily_1_AutoML_1_20240421_24010  0.998094  0.0357054  0.998026              0.00951756  0.0889086  0.00790474\nStackedEnsemble_AllModels_1_AutoML_1_20240421_24010     0.998073  0.0330671  0.997866              0.00837092  0.0842541  0.00709875\nGBM_3_AutoML_1_20240421_24010                           0.997932  0.0337518  0.997449              0.00841532  0.0844416  0.00713038\nGBM_4_AutoML_1_20240421_24010                           0.997888  0.0344127  0.99752               0.00898058  0.0863381  0.00745427\nGBM_5_AutoML_1_20240421_24010                           0.997802  0.0361721  0.997312              0.00862481  0.0850062  0.00722605\nGBM_2_AutoML_1_20240421_24010                           0.99772   0.0362497  0.99692               0.0087179   0.086356   0.00745736\nXGBoost_3_AutoML_1_20240421_24010                       0.997702  0.0417831  0.997323              0.0102275   0.0940119  0.00883824\nXGBoost_2_AutoML_1_20240421_24010                       0.997673  0.0439151  0.997576              0.0117902   0.0994648  0.00989324\nXGBoost_1_AutoML_1_20240421_24010                       0.997624  0.0463814  0.997675              0.0121294   0.101276   0.0102568\nDRF_1_AutoML_1_20240421_24010                           0.997361  0.101367   0.997321              0.0157585   0.142588   0.0203314\nXRT_1_AutoML_1_20240421_24010                           0.918218  0.514455   0.906091              0.154075    0.406199   0.164997\nGLM_1_AutoML_1_20240421_24010                           0.865499  0.451587   0.838467              0.203529    0.381469   0.145519\nDeepLearning_1_AutoML_1_20240421_24010                  0.785593  1.34063    0.809466              0.312888    0.496338   0.246351\n[14 rows x 7 columns]\n\n"}], "source": "# For binary classification, response should be a factor\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\n\n# Run AutoML\naml = H2OAutoML(max_runtime_secs=3600, max_models=12, seed=1)\naml.train(x=x, y=y, training_frame=train)\n\n# View the AutoML Leaderboard\nlb = aml.leaderboard\nprint(lb.head(rows=lb.nrows))  "}, {"cell_type": "code", "execution_count": 27, "id": "68db368c-94fd-4190-9425-d105c6d9f530", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gbm prediction progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (done) 100%\nModelMetricsBinomial: gbm\n** Reported on test data. **\n\nMSE: 0.008079970190256794\nRMSE: 0.0898886544023037\nLogLoss: 0.03344070475011655\nMean Per-Class Error: 0.00991751316689991\nAUC: 0.9987448560323741\nAUCPR: 0.9989567940452997\nGini: 0.9974897120647481\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.506258468100948\n       0     1     Error    Rate\n-----  ----  ----  -------  ---------------\n0      5180  43    0.0082   (43.0/5223.0)\n1      63    5367  0.0116   (63.0/5430.0)\nTotal  5243  5410  0.01     (106.0/10653.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.506258     0.990221  189\nmax f2                       0.241969     0.991909  238\nmax f0point5                 0.738359     0.992677  155\nmax accuracy                 0.506258     0.99005   189\nmax precision                0.999949     1         0\nmax recall                   0.000209259  1         397\nmax specificity              0.999949     1         0\nmax absolute_mcc             0.506258     0.9801    189\nmax min_per_class_accuracy   0.405678     0.989661  204\nmax mean_per_class_accuracy  0.506258     0.990082  189\nmax tns                      0.999949     5223      0\nmax fns                      0.999949     5033      0\nmax fps                      5.63736e-05  5223      399\nmax tps                      0.000209259  5430      397\nmax tnr                      0.999949     1         0\nmax fnr                      0.999949     0.926888  0\nmax fpr                      5.63736e-05  1         399\nmax tpr                      0.000209259  1         397\n\nGains/Lift Table: Avg response rate: 50.97 %, avg score: 50.85 %\ngroup    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ----------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0100441                   0.999965           1.96188     1.96188            1                0.999977     1                           0.999977            0.0197053       0.0197053                  96.1878   96.1878            0.0197053\n2        0.0200882                   0.999948           1.96188     1.96188            1                0.999956     1                           0.999966            0.0197053       0.0394107                  96.1878   96.1878            0.0394107\n3        0.0300385                   0.999929           1.96188     1.96188            1                0.999938     1                           0.999957            0.0195212       0.0589319                  96.1878   96.1878            0.0589319\n4        0.0400826                   0.999914           1.96188     1.96188            1                0.99992      1                           0.999948            0.0197053       0.0786372                  96.1878   96.1878            0.0786372\n5        0.0500329                   0.9999             1.96188     1.96188            1                0.999906     1                           0.99994             0.0195212       0.0981584                  96.1878   96.1878            0.0981584\n6        0.100066                    0.999806           1.96188     1.96188            1                0.999853     1                           0.999896            0.0981584       0.196317                   96.1878   96.1878            0.196317\n7        0.150005                    0.999687           1.96188     1.96188            1                0.999749     1                           0.999847            0.0979742       0.294291                   96.1878   96.1878            0.294291\n8        0.200038                    0.999517           1.96188     1.96188            1                0.999608     1                           0.999787            0.0981584       0.392449                   96.1878   96.1878            0.392449\n9        0.300009                    0.99891            1.96188     1.96188            1                0.999249     1                           0.999608            0.196133        0.588582                   96.1878   96.1878            0.588582\n10       0.399981                    0.996897           1.95451     1.96004            0.996244         0.998147     0.999061                    0.999243            0.195396        0.783978                   95.451    96.0037            0.783212\n11       0.500047                    0.8659             1.92691     1.95341            0.982176         0.982904     0.995682                    0.995973            0.192818        0.976796                   92.6911   95.3408            0.972392\n12       0.600019                    0.00487419         0.222899    1.66508            0.113615         0.101312     0.848717                    0.84691             0.0222836       0.999079                   -77.7101  66.508             0.813937\n13       0.699991                    0.00133426         0.00184214  1.42754            0.000938967      0.00255722   0.727638                    0.72632             0.000184162     0.999263                   -99.8158  42.7538            0.610406\n14       0.799962                    0.000484119        0.00184214  1.24937            0.000938967      0.000834195  0.636822                    0.635656            0.000184162     0.999448                   -99.8158  24.9368            0.406876\n15       0.899934                    0.000175273        0.00552642  1.11119            0.0028169        0.000309864  0.566392                    0.565076            0.000552486     1                          -99.4474  11.1192            0.204097\n16       1                           3.62108e-06        0           1                  0                9.26598e-05  0.509716                    0.508541            0               1                          -100      0                  0\n"}], "source": "# The leader model is the best model\nleader_model = aml.leader\npredictions = leader_model.predict(test)\nperformance = leader_model.model_performance(test)\nprint(performance)"}, {"cell_type": "code", "execution_count": null, "id": "850b40fc-d8a9-4048-a3bb-ce91f6d01e4c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "01e17eb1-4fb7-40f0-bc9b-0a6be31d0268", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "9fd0ca55-b926-4397-89b5-5900b79a9249", "metadata": {}, "source": "# (Run on smaller data) Hyperparameter Tuning"}, {"cell_type": "markdown", "id": "fad7a18d-3e67-402e-a858-62ef132c4673", "metadata": {}, "source": "## 1. Cross Validation"}, {"cell_type": "code", "execution_count": 11, "id": "834f72eb-aeea-4096-93bb-b26a530b254d", "metadata": {}, "outputs": [], "source": "# collect accuarcy and auc for cross validataion\ncv_records = {}"}, {"cell_type": "markdown", "id": "9432e450-137b-4611-b805-424f26777b23", "metadata": {}, "source": "### 1.1 Logistic Regression"}, {"cell_type": "code", "execution_count": 44, "id": "5368e0a7-8c4f-4974-be60-e75725e54396", "metadata": {}, "outputs": [], "source": "# Define parameter grid\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n    .build()"}, {"cell_type": "code", "execution_count": 45, "id": "440f5814-7351-4912-987f-8efd1a187b6b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 10:39:00 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n24/04/21 10:40:05 ERROR breeze.optimize.LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n"}], "source": "# Define evaluator\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\n\n# Set up 5-fold cross-validation\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\n\n# Run cross-validation, and choose the best set of parameters\ncvModel = crossval.fit(train_df)"}, {"cell_type": "code", "execution_count": 46, "id": "e988d2e5-3b28-400e-9adf-0f76002085b5", "metadata": {}, "outputs": [], "source": "# Fetch the best model\nbestModel = cvModel.bestModel\n\n# Make predictions on the test set\ntrain_predictions = bestModel.transform(train_df)\ntest_predictions = bestModel.transform(test_df)"}, {"cell_type": "code", "execution_count": 47, "id": "941b2797-d713-422b-b874-597398f3526a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.9296081277213353\nBest Model Accuracy: 0.9303030303030303\nArea Under ROC: 0.9362521312266379\nArea Under ROC: 0.9368686868686869\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_train}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 48, "id": "af236e05-316f-4fd6-814a-4b2272a97240", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Reg Param: 0.01, \nBest Elastic Net Param: 1.0\n"}], "source": "# view the best model's parameters\nbest_regParam = bestModel._java_obj.getRegParam()\nbest_elasticNetParam = bestModel._java_obj.getElasticNetParam()\nprint(f\"Best Reg Param: {best_regParam}, \\nBest Elastic Net Param: {best_elasticNetParam}\")"}, {"cell_type": "code", "execution_count": null, "id": "e5ab4742-7c1f-4489-8c22-bf0a57c7ac42", "metadata": {}, "outputs": [], "source": "cv_records['logistic regression'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "188b79fe-e635-471a-99c9-ec9138358cce", "metadata": {}, "source": "### 1.2 Decision Tree"}, {"cell_type": "code", "execution_count": 50, "id": "a25e47c9-9eaa-4d6a-9464-c20963527ad5", "metadata": {}, "outputs": [], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n    .addGrid(dt.maxBins, [20, 30, 40]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\ncrossval = CrossValidator(estimator=dt,\n                          estimatorParamMaps=param_grid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = crossval.fit(train_df)"}, {"cell_type": "code", "execution_count": 51, "id": "6a4d0d28-2d3a-44a4-a010-999bd4f2a860", "metadata": {}, "outputs": [], "source": "# Fetch the best model\nbest_model = cv_model.bestModel\n\n# Make predictions on the train,test set\ntrain_predictions = best_model.transform(train_df)\ntest_predictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 52, "id": "d4922270-9862-4ba6-b5b2-3d2f27713291", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.9851233671988389\nBest Model Accuracy: 0.9681818181818181\nArea Under ROC: 0.9870223545005815\nArea Under ROC: 0.970959595959596\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_train}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 53, "id": "45a29dd3-6de5-4298-b4df-a24d833b1376", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Max Depth: 15, Best Max Bins: 30\n"}], "source": "# best model's parameters\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nbest_maxBins = best_model._java_obj.getMaxBins()\nprint(f\"Best Max Depth: {best_maxDepth}, Best Max Bins: {best_maxBins}\")"}, {"cell_type": "code", "execution_count": 45, "id": "3b87b8dd-1039-4567-9747-eea3df677b44", "metadata": {}, "outputs": [], "source": "cv_records['decision tree'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "a7adee0a-089e-4971-b2f3-fb522ed983ae", "metadata": {}, "source": "### 1.3 Random Forest Classifier"}, {"cell_type": "markdown", "id": "813088d7-cade-444e-80df-28bd93f4fecd", "metadata": {}, "source": "#### without early stop"}, {"cell_type": "code", "execution_count": 55, "id": "00ee0b43-9b11-439f-afcb-215b417280c3", "metadata": {}, "outputs": [], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(rf.numTrees, [10, 20, 30]) \\\n    .addGrid(rf.maxDepth, [5, 10]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\ncross_val = CrossValidator(estimator=rf,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)"}, {"cell_type": "code", "execution_count": 56, "id": "ca69b131-8934-46c1-8b00-da03840688ed", "metadata": {}, "outputs": [], "source": "# Fetch the best model\nbest_model = cv_model.bestModel\n\n# Make predictions on the train,test set\ntrain_predictions = best_model.transform(train_df)\ntest_predictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 57, "id": "9f68adf7-cba9-4f1f-a138-2f96e01291a8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.9796806966618288\nBest Model Accuracy: 0.9666666666666667\nArea Under ROC: 0.9824931760088259\nArea Under ROC: 0.9690656565656567\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_train}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 58, "id": "67df48ff-1c9a-4cdf-8057-643b76973a64", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Num Trees: 20, Best Max Depth: 10\n"}], "source": "# best model's parameters\nbest_numTrees = best_model._java_obj.getNumTrees()\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nprint(f\"Best Num Trees: {best_numTrees}, Best Max Depth: {best_maxDepth}\")"}, {"cell_type": "code", "execution_count": 50, "id": "68fbb807-b36f-4ea7-8eea-8d3e1cf1faf8", "metadata": {}, "outputs": [], "source": "cv_records['random forest classifier'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "b2b6d220-f469-4866-add4-74425d920184", "metadata": {}, "source": "#### with early stop"}, {"cell_type": "code", "execution_count": 59, "id": "f2777c7a-c85b-4c61-8b37-2864fd52a7df", "metadata": {}, "outputs": [], "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\nbestauc = float('-inf')\nbestmodel = None\naccuracy_list = []    # accuracy is the matrix that we want to use for early stopping\nstop_tolerance = 0.01 # if accuracy does not imporve by at least 0.001, the training stops\nstop_round = 5        # stop training if there is no improvement in accuracy for 10 consecutive rounds\n\ncondition_met = 0\n\nfor param in param_grid:\n    num_trees, max_depth = param\n    rf = RandomForestClassifier(numTrees=param[num_trees], maxDepth=param[max_depth], labelCol=target_column, featuresCol=\"features\")\n    model = rf.fit(train_df)\n\n    predictions = model.transform(test_df)\n    auc = evaluator.evaluate(predictions)\n\n    if auc > bestauc:\n        bestauc = auc\n        bestmodel = model\n\n    # Add early stopping condition here\n    accuracy = predictions.filter(predictions['Delay'] == predictions['prediction']).count() / predictions.count()\n    \n    if accuracy_list:\n        last_accuracy = accuracy_list[-1]\n        if abs(accuracy-last_accuracy) < stop_tolerance:\n            condition_met += 1\n    accuracy_list.append(accuracy)\n\n    if condition_met == stop_round:\n        break"}, {"cell_type": "code", "execution_count": 60, "id": "db2df1c8-0f10-4332-9b26-410bd094dcba", "metadata": {}, "outputs": [], "source": "# Make predictions on the train,test set\ntrain_predictions = bestmodel.transform(train_df)\ntest_predictions = bestmodel.transform(test_df)"}, {"cell_type": "code", "execution_count": 61, "id": "ddf2a044-d960-4bb4-a64e-a611b4869b7d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training data:\nBest Model Accuracy: 0.9840348330914369\nArea Under ROC: 0.9858386258230126\nTest data:\nBest Model Accuracy: 0.9681818181818181\nArea Under ROC: 0.9696969696969697\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\n\nprint(\"Training data:\")\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Area Under ROC: {auc_train}\")\nprint(\"Test data:\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 62, "id": "bf96fc50-839d-4864-a6a6-96a35d54f48d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Num Trees: 30 \nBest Max Depth: 10\n"}], "source": "# best model's parameters\nbest_numTrees = bestmodel._java_obj.getNumTrees()\nbest_maxDepth = bestmodel._java_obj.getMaxDepth()\nprint(f\"Best Num Trees: {best_numTrees} \\nBest Max Depth: {best_maxDepth}\")"}, {"cell_type": "code", "execution_count": 57, "id": "91c02678-e34b-47fb-8714-a5077616909f", "metadata": {}, "outputs": [], "source": "cv_records['random forest classifier (early stop)'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "826d7c2e-1cb7-4338-bc6e-adda41425f0f", "metadata": {}, "source": "### 1.4 Gradient Boosted Decision Tress"}, {"cell_type": "markdown", "id": "ac9bea48-23c5-4383-8608-1ff174a6c533", "metadata": {}, "source": "#### without early stop"}, {"cell_type": "code", "execution_count": null, "id": "0b7149dd-6ba2-4f1e-8123-cc96efc4bd0c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(gbt.maxDepth, [3, 5, 10]) \\\n    .addGrid(gbt.maxIter, [5, 10, 20]) \\\n    .addGrid(gbt.stepSize, [0.1, 0.05]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\ncross_val = CrossValidator(estimator=gbt,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)"}, {"cell_type": "code", "execution_count": null, "id": "c3ef71a5-ef8e-408c-b5c3-e9dea6d1722b", "metadata": {}, "outputs": [], "source": "# Fetch the best model\nbest_model = cv_model.bestModel\n\n# Make predictions on the train,test set\ntrain_predictions = best_model.transform(train_df)\ntest_predictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": null, "id": "6a302db0-cd25-46f6-9ae1-a7bcf1df3b42", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.9992743105950653\nBest Model Accuracy: 0.9757575757575757\nArea Under ROC: 0.9993792675356921\nArea Under ROC: 0.9766414141414143\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_train}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": null, "id": "929cdc99-05d9-4cbb-ae72-fcd36edf8c6d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Max Depth: 10, Best Max Iter: 20, Best Step Size: 0.1\n"}], "source": "# best model's parameters\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nbest_maxIter = best_model._java_obj.getMaxIter()\nbest_stepSize = best_model._java_obj.getStepSize()\nprint(f\"Best Max Depth: {best_maxDepth}, Best Max Iter: {best_maxIter}, Best Step Size: {best_stepSize}\")"}, {"cell_type": "code", "execution_count": 62, "id": "f7b0ab53-9201-4a1f-baaf-00f553d577d3", "metadata": {}, "outputs": [], "source": "cv_records['gradient boosted decision trees'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "4b7601c5-bf16-4595-bfd2-07982dd8fe91", "metadata": {}, "source": "#### with early stop"}, {"cell_type": "code", "execution_count": 68, "id": "10b2856e-8e17-44c0-9e1b-8720f10b8507", "metadata": {}, "outputs": [], "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\nbestauc = float('-inf')\nbestmodel = None\naccuracy_list = []    # accuracy is the matrix that we want to use for early stopping\nstop_tolerance = 0.01 # if accuracy does not imporve by at least 0.001, the training stops\nstop_round = 5        # stop training if there is no improvement in accuracy for 10 consecutive rounds\n\ncondition_met = 0\n\nfor param in param_grid:\n    max_depth, max_iter, step_size = param\n    gbt = GBTClassifier(maxDepth=param[max_depth], maxIter=param[max_iter], stepSize=param[step_size], labelCol=target_column, featuresCol=\"features\")\n    model = gbt.fit(train_df)\n\n    predictions = model.transform(test_df)\n    auc = evaluator.evaluate(predictions)\n\n    if auc > bestauc:\n        bestauc = auc\n        bestmodel = model\n\n    # Add early stopping condition here\n    accuracy = predictions.filter(predictions['Delay'] == predictions['prediction']).count() / predictions.count()\n    \n    if accuracy_list:\n        last_accuracy = accuracy_list[-1]\n        if abs(accuracy-last_accuracy) < stop_tolerance:\n            condition_met += 1\n    accuracy_list.append(accuracy)\n\n    if condition_met == stop_round:\n        break"}, {"cell_type": "code", "execution_count": 69, "id": "75d7db03-49e6-4f26-837a-66b3f464a61b", "metadata": {}, "outputs": [], "source": "# Make predictions on the train,test set\ntrain_predictions = bestmodel.transform(train_df)\ntest_predictions = bestmodel.transform(test_df)"}, {"cell_type": "code", "execution_count": 70, "id": "f5cc0858-97fd-4708-a49b-fa35888c5358", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training data:\nBest Model Accuracy: 0.9909288824383164\nArea Under ROC: 0.9922408441961514\nTest data:\nBest Model Accuracy: 0.9681818181818181\nArea Under ROC: 0.9659090909090909\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n\nprint(\"Training data:\")\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Area Under ROC: {auc_train}\")\nprint(\"Test data:\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 71, "id": "28373408-d478-400a-9708-bc7382c69f9e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Max Depth: 10 \nBest Max Iter: 5 \nBest Step Size: 0.1\n"}], "source": "# best model's parameters\nbest_maxDepth = bestmodel._java_obj.getMaxDepth()\nbest_maxIter = bestmodel._java_obj.getMaxIter()\nbest_stepSize = bestmodel._java_obj.getStepSize()\nprint(f\"Best Max Depth: {best_maxDepth} \\nBest Max Iter: {best_maxIter} \\nBest Step Size: {best_stepSize}\")"}, {"cell_type": "code", "execution_count": 67, "id": "1fbb208d-815e-4dcf-a88d-928ced5efb6e", "metadata": {}, "outputs": [], "source": "cv_records['gradient boosted decision trees (early stop)'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "fe82b158-bb90-47e3-9d15-6ac412232321", "metadata": {}, "source": "### 1.5 Multilayer Perceptron (MLP)"}, {"cell_type": "markdown", "id": "a9ad6ff7-c5af-44ec-ab42-d6ff95bbf097", "metadata": {}, "source": "#### without early stop"}, {"cell_type": "code", "execution_count": 83, "id": "23735bac-7d83-4b22-a776-8385e558352b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/21 11:08:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 30197.0 (TID 109115) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 1): java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n24/04/21 11:08:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 30197.0 failed 4 times; aborting job\n24/04/21 11:08:27 ERROR org.apache.spark.ml.util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30197.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30197.0 (TID 109122) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 1): java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n24/04/21 11:08:28 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 30198.0 (TID 109124) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 1): java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n24/04/21 11:08:28 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 30198.0 failed 4 times; aborting job\n24/04/21 11:08:28 ERROR org.apache.spark.ml.util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30198.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30198.0 (TID 109134) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 1): java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n"}, {"ename": "Py4JJavaError", "evalue": "An error occurred while calling o68794.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30197.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30197.0 (TID 109122) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 1): java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "Cell \u001b[0;32mIn[83], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m cross_val \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mmlp,\n\u001b[1;32m     10\u001b[0m                            estimatorParamMaps\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m     11\u001b[0m                            evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[1;32m     12\u001b[0m                            numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Run cross-validation to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cv_model \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:687\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    684\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    686\u001b[0m tasks \u001b[38;5;241m=\u001b[39m _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam)\n\u001b[0;32m--> 687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    688\u001b[0m     metrics[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (metric \u001b[38;5;241m/\u001b[39m nFolds)\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/multiprocessing/pool.py:868\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 868\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:687\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    684\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    686\u001b[0m tasks \u001b[38;5;241m=\u001b[39m _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam)\n\u001b[0;32m--> 687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    688\u001b[0m     metrics[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (metric \u001b[38;5;241m/\u001b[39m nFolds)\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:69\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m():\n\u001b[0;32m---> 69\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:69\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:126\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index):\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py:159\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o68794.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30197.0 failed 4 times, most recent failure: Lost task 0.3 in stage 30197.0 (TID 109122) (final-cluster-jy-m.us-central1-c.c.final-project-419701.internal executor 1): java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor148.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1431)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1358)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1422)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1245)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}], "source": "param_grid = ParamGridBuilder() \\\n    .addGrid(mlp.maxIter, [100, 200]) \\\n    .addGrid(mlp.blockSize, [128, 256]) \\\n    .addGrid(mlp.layers, [[114, 5, 4, 2], [114, 6, 3, 2]]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\ncross_val = CrossValidator(estimator=mlp,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)"}, {"cell_type": "code", "execution_count": 84, "id": "b4c69c89-f0e4-44cb-a2c0-554aa6d6f37a", "metadata": {}, "outputs": [], "source": "# Fetch the best model\nbest_model = cv_model.bestModel\n\n# Make predictions on the train,test set\ntrain_predictions = best_model.transform(train_df)\ntest_predictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 85, "id": "b959a2a1-940d-4568-a2b6-ba5c83e1d008", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.8439767779390421\nBest Model Accuracy: 0.8454545454545455\nArea Under ROC: 0.8314269527999372\nArea Under ROC: 0.8295454545454546\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_train}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 86, "id": "052efb7a-e6f4-4760-81a7-e24e611126b4", "metadata": {}, "outputs": [{"ename": "Py4JError", "evalue": "An error occurred while calling o67874.getBlockSize. Trace:\npy4j.Py4JException: Method getBlockSize([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[86], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# best model's parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m best_maxIter \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mgetMaxIter()\n\u001b[0;32m----> 3\u001b[0m best_blockSize \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBlockSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m best_layers \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mgetLayers()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Max Iter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_maxIter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest Block Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_blockSize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest Layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n", "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o67874.getBlockSize. Trace:\npy4j.Py4JException: Method getBlockSize([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"]}], "source": "# best model's parameters\nbest_maxIter = best_model._java_obj.getMaxIter()\nbest_blockSize = best_model._java_obj.getBlockSize()\nbest_layers = best_model._java_obj.getLayers()\nprint(f\"Best Max Iter: {best_maxIter} \\nBest Block Size: {best_blockSize} \\nBest Layers: {best_layers}\")"}, {"cell_type": "code", "execution_count": 72, "id": "c1932277-cb81-44c2-b83e-1deb409d27f6", "metadata": {}, "outputs": [], "source": "cv_records['multilayer perceptron'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "14ff7887-c7c4-49b1-9af5-ac2ce0d4db6a", "metadata": {}, "source": "#### with early stop"}, {"cell_type": "code", "execution_count": 74, "id": "d448ec48-6326-4717-8981-a67200ef93bf", "metadata": {}, "outputs": [{"ename": "TypeError", "evalue": "Invalid param value given for param \"layers\". Could not convert 0.1 to list of ints", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/param/__init__.py:464\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/param/__init__.py:148\u001b[0m, in \u001b[0;36mTypeConverters.toListInt\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mint\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m value]\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to list of ints\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m value)\n", "\u001b[0;31mTypeError\u001b[0m: Could not convert 0.1 to list of ints", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[74], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m param_grid:\n\u001b[1;32m     11\u001b[0m     max_iter, block_size, layer \u001b[38;5;241m=\u001b[39m param\n\u001b[0;32m---> 12\u001b[0m     mlp \u001b[38;5;241m=\u001b[39m \u001b[43mMultilayerPerceptronClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     model \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mfit(train_df)\n\u001b[1;32m     15\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_df)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/__init__.py:114\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/classification.py:2609\u001b[0m, in \u001b[0;36mMultilayerPerceptronClassifier.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, tol, seed, layers, blockSize, stepSize, solver, initialWeights, probabilityCol, rawPredictionCol)\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m   2607\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.classification.MultilayerPerceptronClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m   2608\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m-> 2609\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/__init__.py:114\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/classification.py:2625\u001b[0m, in \u001b[0;36mMultilayerPerceptronClassifier.setParams\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, tol, seed, layers, blockSize, stepSize, solver, initialWeights, probabilityCol, rawPredictionCol)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   2619\u001b[0m \u001b[38;5;124;03m          maxIter=100, tol=1e-6, seed=None, layers=None, blockSize=128, stepSize=0.03, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;124;03mSets params for MultilayerPerceptronClassifier.\u001b[39;00m\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2624\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m-> 2625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/ml/param/__init__.py:466\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m             value \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtypeConverter(value)\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramMap[p] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n", "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"layers\". Could not convert 0.1 to list of ints"]}], "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\nbestauc = float('-inf')\nbestmodel = None\naccuracy_list = []    # accuracy is the matrix that we want to use for early stopping\nstop_tolerance = 0.01 # if accuracy does not imporve by at least 0.001, the training stops\nstop_round = 5        # stop training if there is no improvement in accuracy for 10 consecutive rounds\n\ncondition_met = 0\n\nfor param in param_grid:\n    max_iter, block_size, layer = param\n    mlp = MultilayerPerceptronClassifier(maxIter=param[max_iter], blockSize=param[block_size], layers=param[layer], labelCol=target_column, featuresCol=\"features\")\n    model = mlp.fit(train_df)\n\n    predictions = model.transform(test_df)\n    auc = evaluator.evaluate(predictions)\n\n    if auc > bestauc:\n        bestauc = auc\n        bestmodel = model\n\n    # Add early stopping condition\n    accuracy = predictions.filter(predictions['Delay'] == predictions['prediction']).count() / predictions.count()\n    \n    if accuracy_list:\n        last_accuracy = accuracy_list[-1]\n        if abs(accuracy-last_accuracy) < stop_tolerance:\n            condition_met += 1\n    accuracy_list.append(accuracy)\n\n    if condition_met == stop_round:\n        break"}, {"cell_type": "code", "execution_count": null, "id": "16111de3-ec6f-463b-bfda-661ab3753799", "metadata": {}, "outputs": [], "source": "# Make predictions on the train,test set\ntrain_predictions = bestmodel.transform(train_df)\ntest_predictions = bestmodel.transform(test_df)"}, {"cell_type": "code", "execution_count": null, "id": "d23b2e54-405f-4994-aabc-6240c1a546bb", "metadata": {}, "outputs": [], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n\nprint(\"Training data:\")\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Area Under ROC: {auc_train}\")\nprint(\"Test data:\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": null, "id": "25d6f21d-9dbb-45b9-8c20-e22ef2f0c40a", "metadata": {}, "outputs": [], "source": "# best model's parameters\nbest_maxIter = bestmodel._java_obj.getMaxIter()\nbest_blockSize = bestmodel._java_obj.getBlockSize()\nbest_layers = bestmodel._java_obj.getLayers()\nprint(f\"Best Max Iter: {best_maxIter} \\nBest Block Size: {best_blockSize} \\nBest Layers: {best_layers}\")"}, {"cell_type": "code", "execution_count": null, "id": "48eb79c1-0bd1-44da-b6ac-0fb4ac7e1f91", "metadata": {}, "outputs": [], "source": "cv_records['multilayer perceptron (early stop)'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "0746de62-c6bb-4d75-848e-dfb91f587113", "metadata": {}, "source": "### 1.6 Linear Support Vector Machine"}, {"cell_type": "markdown", "id": "522bb7e5-8a47-46c5-9d92-cd74d5dd2c7c", "metadata": {}, "source": "#### without early stop"}, {"cell_type": "code", "execution_count": 75, "id": "b0b89208-bb89-4a56-91fe-4a8348a8c2e3", "metadata": {}, "outputs": [], "source": "param_grid = ParamGridBuilder() \\\n    .addGrid(lsvc.regParam, [0.1, 0.01]) \\\n    .addGrid(lsvc.maxIter, [10, 20]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\ncross_val = CrossValidator(estimator=lsvc,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)"}, {"cell_type": "code", "execution_count": 76, "id": "13f518d7-e940-4c97-bfaf-2e7d85bacc5e", "metadata": {}, "outputs": [], "source": "# Fetch the best model\nbest_model = cv_model.bestModel\n\n# Make predictions on the train,test set\ntrain_predictions = best_model.transform(train_df)\ntest_predictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 77, "id": "3d2ae31f-fca1-41b3-a6b7-d01371027703", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.8439767779390421\nBest Model Accuracy: 0.8454545454545455\nArea Under ROC: 0.8314269527999372\nArea Under ROC: 0.8295454545454546\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(test_predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc_train}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 78, "id": "23bc384e-f77e-4270-a4b3-53b177388eb6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Reg Param: 0.01 \nBest Max Iter: 20\n"}], "source": "# best model's parameters\nbest_regParam = best_model._java_obj.getRegParam()\nbest_maxIter = best_model._java_obj.getMaxIter()\nprint(f\"Best Reg Param: {best_regParam} \\nBest Max Iter: {best_maxIter}\")"}, {"cell_type": "code", "execution_count": 81, "id": "310231a0-ace9-40e4-81d7-81aec82c0749", "metadata": {}, "outputs": [], "source": "cv_records['linear support vector machine'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "e06057fe-e028-4672-8133-316d94895672", "metadata": {}, "source": "#### with early stop"}, {"cell_type": "code", "execution_count": 79, "id": "f036fea4-02e9-43af-bf31-ef0b3d7efa6a", "metadata": {}, "outputs": [], "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\nbestauc = float('-inf')\nbestmodel = None\naccuracy_list = []    # accuracy is the matrix that we want to use for early stopping\nstop_tolerance = 0.01 # if accuracy does not imporve by at least 0.001, the training stops\nstop_round = 5        # stop training if there is no improvement in accuracy for 10 consecutive rounds\n\ncondition_met = 0\n\nfor param in param_grid:\n    reg_param,max_iter = param\n    lsvc = LinearSVC(maxIter=param[max_iter], regParam=param[reg_param], labelCol=target_column, featuresCol=\"features\")\n    model = lsvc.fit(train_df)\n\n    predictions = model.transform(test_df)\n    auc = evaluator.evaluate(predictions)\n\n    if auc > bestauc:\n        bestauc = auc\n        bestmodel = model\n\n    # Add early stopping condition here\n    accuracy = predictions.filter(predictions['Delay'] == predictions['prediction']).count() / predictions.count()\n    \n    if accuracy_list:\n        last_accuracy = accuracy_list[-1]\n        if abs(accuracy-last_accuracy) < stop_tolerance:\n            condition_met += 1\n    accuracy_list.append(accuracy)\n\n    if condition_met == stop_round:\n        break"}, {"cell_type": "code", "execution_count": 80, "id": "0eaf289b-a52d-46f1-8b3e-c17d7a7f2d1d", "metadata": {}, "outputs": [], "source": "# Make predictions on the train,test set\ntrain_predictions = bestmodel.transform(train_df)\ntest_predictions = bestmodel.transform(test_df)"}, {"cell_type": "code", "execution_count": 81, "id": "cde5fc1f-e4ef-496d-84d9-e42eedd095c7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training data:\nBest Model Accuracy: 0.8439767779390421\nArea Under ROC: 0.8314269527999372\nTest data:\nBest Model Accuracy: 0.8454545454545455\nArea Under ROC: 0.8295454545454546\n"}], "source": "# Evaluate the best model's performance\naccuracy_train = train_predictions.filter(train_predictions['Delay'] == train_predictions['prediction']).count() / train_predictions.count()\naccuracy_test = test_predictions.filter(test_predictions['Delay'] == test_predictions['prediction']).count() / test_predictions.count()\n# Calculate the area under the ROC curve (AUC)\nauc_train = evaluator.evaluate(train_predictions, {evaluator.metricName: \"areaUnderROC\"})\nauc_test = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n\nprint(\"Training data:\")\nprint(f\"Best Model Accuracy: {accuracy_train}\")\nprint(f\"Area Under ROC: {auc_train}\")\nprint(\"Test data:\")\nprint(f\"Best Model Accuracy: {accuracy_test}\")\nprint(f\"Area Under ROC: {auc_test}\")"}, {"cell_type": "code", "execution_count": 82, "id": "fd3a213a-36e7-461b-8cab-de39130df51d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Reg Param: 0.01 \nBest Max Iter: 20\n"}], "source": "# best model's parameters\nbest_regParam = bestmodel._java_obj.getRegParam()\nbest_maxIter = bestmodel._java_obj.getMaxIter()\nprint(f\"Best Reg Param: {best_regParam} \\nBest Max Iter: {best_maxIter}\")"}, {"cell_type": "code", "execution_count": null, "id": "3d63e78d-2a77-4b49-b37a-a77c53819e99", "metadata": {}, "outputs": [], "source": "cv_records['linear support vector machine (early stop)'] = [accuracy_train, auc_train, accuracy_test, auc_test]"}, {"cell_type": "markdown", "id": "e6e7201c-4520-47ae-85c1-af1594bf4945", "metadata": {}, "source": "## 2.Feature importances analysis"}, {"cell_type": "code", "execution_count": 10, "id": "dd40c037-0a07-46a7-9be8-dbcc7fafd41e", "metadata": {}, "outputs": [], "source": "# Set a threshold\nthreshold = 0.001"}, {"cell_type": "markdown", "id": "183ffd38-420a-48b2-8251-0618e6b3971c", "metadata": {}, "source": "### 2.1 Logistic Regression\n"}, {"cell_type": "code", "execution_count": 16, "id": "2efe0696-861f-4149-bdcb-1e28ed934cc5", "metadata": {}, "outputs": [], "source": "# Assuming model is your trained LogisticRegressionModel\ncoefficients = lr_Model.coefficients\nintercept = lr_Model.intercept"}, {"cell_type": "code", "execution_count": 17, "id": "18018511-2929-4825-b856-0d10038bdd47", "metadata": {}, "outputs": [], "source": "lr_coef = list(map(lambda row: abs(row), list(coefficients)))"}, {"cell_type": "code", "execution_count": 18, "id": "09d5851f-b2af-45c1-a79f-d1f5a9a9f0b2", "metadata": {}, "outputs": [], "source": "# Create a list of tuples containing the original index and the value\nindexed_list = [(index, value) for index, value in enumerate(list(coefficients))]\n\n# Sort the indexed list based on the values\nsorted_indexed_list = sorted(indexed_list, key=lambda x: x[1])\n\n# Extract the indices after sorting\nsorted_indices = [index for index, _ in sorted_indexed_list]"}, {"cell_type": "code", "execution_count": 19, "id": "64eee0f1-55a6-429f-add7-1840fc444624", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['arrival_airport_code_bit_0', 'arrival_airport_code_bit_3', 'arrival_airport_code_bit_5', 'airline_code_bit_0', 'airline_code_bit_3', 'airline_code_bit_4', 'airline_code_bit_5', 'scheduled_aircraft_type_bit_0', 'scheduled_aircraft_type_bit_1', 'scheduled_aircraft_type_bit_2', 'scheduled_aircraft_type_bit_3', 'scheduled_aircraft_type_bit_4', 'scheduled_aircraft_type_bit_5', 'arrival_airport_bit_7', 'arrival_airport_bit_8', 'aircraft_id_bit_0', 'aircraft_id_bit_1', 'aircraft_id_bit_2', 'aircraft_id_bit_7', 'aircraft_id_bit_8']\n"}], "source": "# Get the top 20 features with the higher coefficients\nlr_top20_ind = [i<20 for i in sorted_indices]\n\n# Use list comprehension to filter column names based on boolean values\nlr_important_features = [col_name for col_name, bool_val in zip(sampled_df.columns, lr_top20_ind) if bool_val]\n\nprint(lr_important_features)  # Output: ['id', 'age']"}, {"cell_type": "markdown", "id": "fd0071c2-e10d-4539-addc-1b9aea60a68d", "metadata": {}, "source": "### 2.2 Decision Tree"}, {"cell_type": "code", "execution_count": 21, "id": "340c3c11-4299-4f2f-b76b-681839654f38", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Important Features: ['departure_airport_avg_bearing', 'arrival_airport_max_temperature']\n"}], "source": "# Get feature importances\ndt_feature_importances = dt_model.featureImportances\n\n# Print feature importances\n# print(\"Feature Importances:\")\n# for feature, importance in zip(feature_columns, feature_importances):\n#     print(f\"{feature}: {importance}\")\n\n# Optionally, select the most important features based on a threshold\nthreshold = 0.001\ndt_important_features = [feature for feature, importance in zip(feature_columns, dt_feature_importances) if importance >= threshold]\nprint(\"Important Features:\", dt_important_features)"}, {"cell_type": "markdown", "id": "38de70d7-0091-40db-813c-a82075fddd7c", "metadata": {}, "source": "The feature importance shows that it cannot get the important feature than label itself for Decision Trees model."}, {"cell_type": "markdown", "id": "111e6722-5d67-42ec-b0c2-f642f943733e", "metadata": {}, "source": "### 2.3 Random Forest Classifier"}, {"cell_type": "code", "execution_count": 29, "id": "075f712c-e1ee-4d83-a97b-2ba59fa8173e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Important Features:\n"}, {"data": {"text/plain": "['arrival_airport_avg_bearing',\n 'arrival_airport_min_temperature',\n 'arrival_airport_max_knots',\n 'arrival_airport_max_temperature',\n 'scheduled_air_time',\n 'scheduled_block_time',\n 'arrival_airport_code_bit_1',\n 'airline_code_bit_2',\n 'departure_airport_bit_0',\n 'departure_airport_bit_2',\n 'departure_airport_bit_8',\n 'arrival_airport_bit_2',\n 'arrival_airport_bit_5',\n 'arrival_airport_bit_7',\n 'aircraft_id_bit_1',\n 'legacy_route_bit_12',\n 'legacy_route_bit_13']"}, "execution_count": 29, "metadata": {}, "output_type": "execute_result"}], "source": "# Get feature importances\nrf_feature_importances = rf_model.featureImportances\n\n# Print feature importances\n# print(\"Feature Importances:\")\n# for feature, importance in zip(feature_columns, feature_importances):\n#     print(f\"{feature}: {importance}\")\n\n# Optionally, select the most important features based on a threshold\nthreshold = 0.01\nrf_important_features = [feature for feature, importance in zip(feature_columns, rf_feature_importances) if importance >= threshold]\nprint(\"Important Features:\")\nrf_important_features"}, {"cell_type": "markdown", "id": "a8b85d7a-5604-4972-8584-bba0a6daa0fd", "metadata": {}, "source": "#### 2.3.1 Important freature selection"}, {"cell_type": "code", "execution_count": null, "id": "a0277dc0-d3f8-431c-a4ef-a585a611631d", "metadata": {}, "outputs": [], "source": "# Use important features for further analysis or model training\nselected_data = sampled_df.select(rf_important_features + [target_column])"}, {"cell_type": "code", "execution_count": 30, "id": "3d780879-beb6-49f9-8303-466517b7f7c0", "metadata": {}, "outputs": [], "source": "sampled_df = selected_data\nfeature_df = sampled_df.drop('Delay')"}, {"cell_type": "code", "execution_count": 31, "id": "ed2bdf51-ca64-455d-a965-7031ac7c02ac", "metadata": {}, "outputs": [], "source": "######## Process the data & do train-test split #######\n\n# Define feature columns\nfeature_columns = feature_df.columns  # Assuming the last column is the target\n\n# Define the target column\ntarget_column = 'Delay'\n\n# Assemble features into a single vector column\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n#sampled_df_model = assembler.transform(sampled_df)\n# Define a simple pipeline\npipeline = Pipeline(stages=[assembler])\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(sampled_df)\n\n# Transform the data with the pipeline\nsampled_df_transformed = pipelineModel.transform(sampled_df)\n# Select features and target column for modeling\nsampled_df_model = sampled_df_transformed.select(\"features\", target_column)\n\n# Split data into training and test sets\ntrain_df, test_df = sampled_df_model.randomSplit([train_split, test_split])"}, {"cell_type": "code", "execution_count": 32, "id": "59f150b4-db90-4039-94ef-fba6abe536f4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 1.0\nTest Accuracy: 1.0\nArea Under ROC: 1.0\n"}], "source": "######## train rf again ########\n# Train a RandomForestClassifier model\nrf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\")\nrf_model = rf.fit(train_df)\n\n######## get the accuracy & AUC for model trained on important features ########\n# Evaluate the model on the training set by accuracy\ntrain_predictions = rf_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = rf_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "id": "74904c0b-79e5-4093-88dc-e90c28fc5d91", "metadata": {}, "source": "After select those most important features, we got the train acc, test acc, and test auc all being 1. It is good, but it become 1 might not just because of the selection of the important features. It **might also caused by the new train-test split** after we select those important features."}, {"cell_type": "markdown", "id": "409e634a-29b5-48a2-91a6-ebea575f3303", "metadata": {}, "source": "### 2.4 Gradient Boosted Decsion Tree"}, {"cell_type": "code", "execution_count": 35, "id": "b538966e-5f08-4f7a-a688-546832100e1b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Important Features: ['arrival_airport_avg_bearing', 'arrival_airport_avg_knots', 'scheduled_air_time', 'airline_code_bit_2', 'departure_airport_bit_2', 'departure_airport_bit_8', 'arrival_airport_bit_2', 'arrival_airport_bit_5', 'legacy_route_bit_13']\n"}], "source": "# Get feature importances\ngbt_feature_importances = gbt_model.featureImportances\n\n# Print feature importances\n# print(\"Feature Importances:\")\n# for feature, importance in zip(feature_columns, feature_importances):\n#     print(f\"{feature}: {importance}\")\n\n# Optionally, select the most important features based on a threshold\nthreshold = 0.01\ngbt_important_features = [feature for feature, importance in zip(feature_columns, gbt_feature_importances) if importance >= threshold]\nprint(\"Important Features:\", gbt_important_features)"}, {"cell_type": "markdown", "id": "164d062e-4b27-497f-b86c-d9e31a1feca9", "metadata": {}, "source": "The feature importance shows that it cannot get the important feature than label itself for Gradient Boosted Decision Trees model."}, {"cell_type": "code", "execution_count": 36, "id": "c1d24865-160c-4724-a7f0-49216c8ac28b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Important Features: ['arrival_airport_avg_bearing', 'arrival_airport_avg_knots', 'arrival_airport_min_temperature', 'scheduled_air_time', 'airline_code_bit_2', 'departure_airport_bit_2', 'departure_airport_bit_8', 'arrival_airport_bit_2', 'arrival_airport_bit_5', 'legacy_route_bit_13']\n"}], "source": "# Optionally, select the most important features based on a threshold\nthreshold = 0.001\ngbt_important_features = [feature for feature, importance in zip(feature_columns, gbt_feature_importances) if importance >= threshold]\nprint(\"Important Features:\", gbt_important_features)"}, {"cell_type": "code", "execution_count": null, "id": "01c35cca-491f-4712-b7dc-aa7fd1f3428c", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "7d0d54dc-2020-4e34-94c3-aa9cb079018f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "0eb7cf1a-c02c-4c2d-8fdc-3c90d11c1931", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "f48a6c8d-9912-42fb-a7e1-1fee7e4a7dc4", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "f47dcb7e-1b81-4f16-9702-bc74809c90a0", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "41c8ad36-412d-43a7-8a89-9b581cd5ab7e", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "f330379c-e229-4c4c-91ea-a644bafc4c1f", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "e536fe99-416f-4caa-b082-50ffd524b224", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "456b2021-cbad-4d80-89ec-599f3679682e", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "397ab71c-801c-4b37-b9aa-7dca07d95c84", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}