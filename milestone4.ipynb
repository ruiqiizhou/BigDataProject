{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Milestone 4"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession, Row\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "###### Define train-test split ######\ntrain_split = 0.8\ntest_split = 0.2"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "###### Set up session & data ######\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"DecisionTreeJob\").getOrCreate()\n\n# Import the data as an RDD\ncsv_rdd = spark.sparkContext.textFile(\"gs://bigdataprojectbucketsnew1/flight_encoded_rdd.csv/part-00000-555f6d26-eb7c-4074-8f48-1da198d98ad1-c000.csv\")\n\n# Convert the RDD to a DataFrame\ncsv_rows = csv_rdd.map(lambda line: line.split(\",\"))\nheader = csv_rows.first()  # Extract header\ncsv_data = csv_rows.filter(lambda row: row != header).map(lambda row: Row(**{header[i]: float(row[i]) for i in range(len(header))}))"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "# Create a DataFrame from the RDD of Rows\nsampled_df = spark.createDataFrame(csv_data)\n\n# Limit the number of rows to 100\n#sampled_df = sampled_df.limit(50)"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": "3202"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "sampled_df.count()\n#3202"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "# Define feature columns\nfeature_columns = sampled_df.columns[:-1]  # Assuming the last column is the target\n\n# Define the target column\ntarget_column = 'Delay'"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "# Assemble features into a single vector column\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n#sampled_df_model = assembler.transform(sampled_df)\n# Define a simple pipeline\npipeline = Pipeline(stages=[assembler])\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(sampled_df)\n\n# Transform the data with the pipeline\nsampled_df_transformed = pipelineModel.transform(sampled_df)\n# Select features and target column for modeling\nsampled_df_model = sampled_df_transformed.select(\"features\", target_column)\n\n# Split data into training and test sets\ntrain_df, test_df = sampled_df_model.randomSplit([train_split, test_split])"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"data": {"text/plain": "115"}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": "len(sampled_df.columns)"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[features: vector, Delay: double]"}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "train_df"}, {"cell_type": "markdown", "metadata": {}, "source": "# 1. Logistic Regression"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/09 10:22:50 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n24/04/09 10:22:50 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"}], "source": "# Train a logistic regression model on the subset of data\nlr = LogisticRegression(featuresCol='features', labelCol=target_column)\nlr_Model = lr.fit(train_df)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 1.0\nTest Accuracy: 1.0\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = lr_Model.transform(train_df)\n# by accuracy\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {accuracy}\")\n\n# Evaluate the model on the test set by accuracy\npredictions = lr_Model.transform(test_df)\n# by accuracy\naccuracy = evaluator_acc.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 1.1 Hyperparameter Tuning"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "# Define parameter grid\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n    .build()"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/09 10:35:10 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n24/04/09 10:35:10 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n                                                                                \r"}], "source": "# Define evaluator\n#evaluator = BinaryClassificationEvaluator(labelCol=\"Delay\", metricName=\"accuracy\")\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Delay\")\n\n# Set up 5-fold cross-validation\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\n# Run cross-validation, and choose the best set of parameters\ncvModel = crossval.fit(train_df)\n\n# Fetch the best model\nbestModel = cvModel.bestModel\n\n# Make predictions on the test set\npredictions = bestModel.transform(test_df)"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 1.0\n"}], "source": "# Calculate the area under the ROC curve (AUC)\nauc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 1.0\nBest Max Depth: 0.01, Best Max Bins: 0.0\n"}], "source": "# Evaluate the best model's performance\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Best Model Accuracy: {accuracy}\")\n\n# Optional: You can also view the best model's parameters\nbest_regParam = bestModel._java_obj.getRegParam()\nbest_elasticNetParam = bestModel._java_obj.getElasticNetParam()\nprint(f\"Best Reg Param: {best_regParam}, Best Elastic Net Param: {best_elasticNetParam}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# 2. Decision Tree"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 1.0\nTest Accuracy: 1.0\nArea Under ROC: 1.0\n"}], "source": "###### Modeling ######\n# Initialize the DecisionTreeClassifier\ndt = DecisionTreeClassifier(labelCol=target_column, featuresCol=\"features\")\n\n# Train the decision tree model\ndt_model = dt.fit(train_df)\n\n# Evaluate the model on the training set by accuracy\ntrain_predictions = dt_model.transform(train_df)\n# by accuracy\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {accuracy}\")\n\n# Evaluate the model on the test set by accuracy\npredictions = dt_model.transform(test_df)\n# by accuracy\naccuracy = evaluator_acc.evaluate(predictions)\nprint(f\"Test Accuracy: {accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\nauc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2.1 Hyperparameter Tuning"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n    .addGrid(dt.maxBins, [20, 30, 40]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Delay\", predictionCol=\"prediction\", metricName=\"accuracy\")\ncrossval = CrossValidator(estimator=dt,\n                          estimatorParamMaps=param_grid,\n                          evaluator=evaluator,\n                          numFolds=3)  # You can adjust the number of folds as needed\n\n# Run cross-validation to find the best hyperparameters\ncv_model = crossval.fit(train_df)\n\n# Make predictions on the test data using the best model\nbest_model = cv_model.bestModel\npredictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 1.0\nBest Max Depth: 5, Best Max Bins: 20\n"}], "source": "# Evaluate the best model's performance\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Best Model Accuracy: {accuracy}\")\n\n# Optional: You can also view the best model's parameters\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nbest_maxBins = best_model._java_obj.getMaxBins()\nprint(f\"Best Max Depth: {best_maxDepth}, Best Max Bins: {best_maxBins}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# 3. Random Forest Classifier"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": "#RandomForestClassifier"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": "# Train a RandomForestClassifier model\nrf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\")\nrf_model = rf.fit(train_df)"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 0.9737151824244802\nTest Accuracy: 0.9509954058192955\nArea Under ROC: 0.9993418743418743\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = rf_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = rf_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3.1 Feature importances analysis"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Feature Importances:\ndeparture_airport_avg_altitude: 0.0\ndeparture_airport_avg_bearing: 0.014632612571395651\ndeparture_airport_avg_knots: 0.009129679230682138\ndeparture_airport_avg_temperature: 0.018308213260391063\ndeparture_airport_min_altitude: 0.0\ndeparture_airport_min_bearing: 0.008913779802424453\ndeparture_airport_min_knots: 0.0\ndeparture_airport_min_temperature: 0.023004848269726666\ndeparture_airport_max_altitude: 0.0\ndeparture_airport_max_bearing: 0.0\ndeparture_airport_max_knots: 0.008181979464980732\ndeparture_airport_max_temperature: 0.004083018514116807\narrival_airport_avg_altitude: 0.0\narrival_airport_avg_bearing: 0.0038677992100691998\narrival_airport_avg_knots: 0.008264721418263976\narrival_airport_avg_temperature: 0.02530843404603288\narrival_airport_min_altitude: 0.0\narrival_airport_min_bearing: 0.0005166539383457392\narrival_airport_min_knots: 0.0\narrival_airport_min_temperature: 0.005740648292316566\narrival_airport_max_altitude: 0.0\narrival_airport_max_bearing: 0.0\narrival_airport_max_knots: 0.003065669424256753\narrival_airport_max_temperature: 0.006855564257804132\nflight_number: 0.013873953313452137\nscheduled_air_time: 0.018017122125620087\nscheduled_block_time: 0.01759398781896223\nDelay: 0.5406208380541289\ndeparture_airport_code_bit_0: 0.0007225946239117053\ndeparture_airport_code_bit_1: 0.0\ndeparture_airport_code_bit_2: 0.0\ndeparture_airport_code_bit_3: 0.0\ndeparture_airport_code_bit_4: 0.001522604310654459\ndeparture_airport_code_bit_5: 0.0022571200142971594\ndeparture_airport_code_bit_6: 0.015075416563092636\narrival_airport_code_bit_0: 0.0\narrival_airport_code_bit_1: 0.0\narrival_airport_code_bit_2: 0.0\narrival_airport_code_bit_3: 0.0\narrival_airport_code_bit_4: 0.004892454497992156\narrival_airport_code_bit_5: 0.004442940326724719\narrival_airport_code_bit_6: 6.565917554762161e-05\nairline_code_bit_0: 0.0019328802379803875\nairline_code_bit_1: 0.0009800885373703742\nairline_code_bit_2: 6.688918438113821e-05\nairline_code_bit_3: 0.000292178577710369\nairline_code_bit_4: 0.0\nairline_code_bit_5: 0.0\nscheduled_aircraft_type_bit_0: 0.002220304421252465\nscheduled_aircraft_type_bit_1: 0.0\nscheduled_aircraft_type_bit_2: 0.005440662023658068\nscheduled_aircraft_type_bit_3: 0.0\nscheduled_aircraft_type_bit_4: 0.0\nscheduled_aircraft_type_bit_5: 0.00570045579783284\nicao_aircraft_type_actual_bit_0: 0.0007511392538639766\nicao_aircraft_type_actual_bit_1: 0.0\nicao_aircraft_type_actual_bit_2: 0.0\nicao_aircraft_type_actual_bit_3: 0.0\nicao_aircraft_type_actual_bit_4: 0.0015234545314593913\nicao_aircraft_type_actual_bit_5: 0.0\ndeparture_airport_bit_0: 0.0\ndeparture_airport_bit_1: 0.0\ndeparture_airport_bit_2: 0.002320573284054385\ndeparture_airport_bit_3: 0.0\ndeparture_airport_bit_4: 0.0006163046060873096\ndeparture_airport_bit_5: 0.027574355977885833\ndeparture_airport_bit_6: 0.016763985324084584\ndeparture_airport_bit_7: 0.07274815907143618\ndeparture_airport_bit_8: 0.0161928391284522\narrival_airport_bit_0: 0.0\narrival_airport_bit_1: 0.006368909727344535\narrival_airport_bit_2: 0.00022111884845294987\narrival_airport_bit_3: 0.0\narrival_airport_bit_4: 0.0002145735342919064\narrival_airport_bit_5: 0.0007086004686704283\narrival_airport_bit_6: 0.0006067582523499543\narrival_airport_bit_7: 0.01614578318884821\narrival_airport_bit_8: 0.0\naircraft_id_bit_0: 0.0003729156300907226\naircraft_id_bit_1: 0.006043695236792161\naircraft_id_bit_2: 0.0005659762697115497\naircraft_id_bit_3: 0.0\naircraft_id_bit_4: 0.0\naircraft_id_bit_5: 0.00037604350670194435\naircraft_id_bit_6: 0.0\naircraft_id_bit_7: 0.0\naircraft_id_bit_8: 0.0010015190051519689\naircraft_id_bit_9: 0.0032776985623155343\naircraft_id_bit_10: 0.005784005435416685\naircraft_id_bit_11: 0.00810524557738122\naircraft_id_bit_12: 0.0033070490486494042\nlegacy_route_bit_0: 0.0006248202878882934\nlegacy_route_bit_1: 0.000600300126108562\nlegacy_route_bit_2: 0.00398203351019589\nlegacy_route_bit_3: 0.0\nlegacy_route_bit_4: 0.0\nlegacy_route_bit_5: 0.005600401159255492\nlegacy_route_bit_6: 0.0024133071445343957\nlegacy_route_bit_7: 0.0\nlegacy_route_bit_8: 0.000990431278116749\nlegacy_route_bit_9: 0.0006744136039538759\nlegacy_route_bit_10: 0.005161432481600711\nlegacy_route_bit_11: 0.0036784358228551966\nlegacy_route_bit_12: 0.0\nlegacy_route_bit_13: 0.0\nlegacy_route_bit_14: 0.0007617272657296956\nlegacy_route_bit_15: 0.0\nlegacy_route_bit_16: 0.0018330916108061223\nlegacy_route_bit_17: 0.0\nlegacy_route_bit_18: 0.0064971309340857335\nbit_0: 0.0\nbit_1: 0.0\nbit_2: 0.0\nbit_3: 0.0\nImportant Features: ['departure_airport_avg_bearing', 'departure_airport_avg_temperature', 'departure_airport_min_temperature', 'arrival_airport_avg_temperature', 'flight_number', 'scheduled_air_time', 'scheduled_block_time', 'Delay', 'departure_airport_code_bit_6', 'departure_airport_bit_5', 'departure_airport_bit_6', 'departure_airport_bit_7', 'departure_airport_bit_8', 'arrival_airport_bit_7']\n"}], "source": "# Get feature importances\nfeature_importances = rf_model.featureImportances\n\n# Print feature importances\nprint(\"Feature Importances:\")\nfor feature, importance in zip(feature_columns, feature_importances):\n    print(f\"{feature}: {importance}\")\n\n# Optionally, select the most important features based on a threshold\nthreshold = 0.01\nimportant_features = [feature for feature, importance in zip(feature_columns, feature_importances) if importance >= threshold]\nprint(\"Important Features:\", important_features)"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3.2 Important freature selection"}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": "# Use important features for further analysis or model training\nselected_data = sampled_df.select(important_features + [target_column])"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": "######## Process the data & do train-test split #######\n\n# Define feature columns\nfeature_columns = selected_data.columns[:-1]  # Assuming the last column is the target\n\n# Define the target column\ntarget_column = 'Delay'\n\n# Assemble features into a single vector column\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n#sampled_df_model = assembler.transform(sampled_df)\n# Define a simple pipeline\npipeline = Pipeline(stages=[assembler])\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(sampled_df)\n\n# Transform the data with the pipeline\nsampled_df_transformed = pipelineModel.transform(sampled_df)\n# Select features and target column for modeling\nsampled_df_model = sampled_df_transformed.select(\"features\", target_column)\n\n# Split data into training and test sets\ntrain_df, test_df = sampled_df_model.randomSplit([train_split, test_split])"}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 1.0\nTest Accuracy: 1.0\nArea Under ROC: 1.0\n"}], "source": "######## train rf again ########\n# Train a RandomForestClassifier model\nrf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\")\nrf_model = rf.fit(train_df)\n\n######## get the accuracy & AUC for model trained on important features ########\n# Evaluate the model on the training set by accuracy\ntrain_predictions = rf_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = rf_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "After select those most important features, we got the train acc, test acc, and test auc all being 1. It is good, but it become 1 might not just because of the selection of the important features. It **might also caused by the new train-test split** after we select those important features."}, {"cell_type": "markdown", "metadata": {}, "source": "## 3.3 Hyperparameter Tuning"}, {"cell_type": "markdown", "metadata": {}, "source": "### 3.3.1 After important feature selection"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/09 19:25:30 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n24/04/09 19:25:30 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n"}], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(rf.numTrees, [10, 20, 30]) \\\n    .addGrid(rf.maxDepth, [5, 10]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ncross_val = CrossValidator(estimator=rf,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)\n\n# Make predictions on the test data using the best model\nbest_model = cv_model.bestModel\npredictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 1.0\nBest Num Trees: 10, Best Max Depth: 5\n"}], "source": "# Evaluate the best model's performance\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Best Model Accuracy: {accuracy}\")\n\n# Optional: You can also view the best model's parameters\nbest_numTrees = best_model._java_obj.getNumTrees()\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nprint(f\"Best Num Trees: {best_numTrees}, Best Max Depth: {best_maxDepth}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### 3.3.2 With full features"}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": "# Define feature columns\nfeature_columns = sampled_df.columns[:-1]  # Assuming the last column is the target\n\n# Define the target column\ntarget_column = 'Delay'\n\n# Assemble features into a single vector column\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\n# Define a simple pipeline\npipeline = Pipeline(stages=[assembler])\n\n# Fit the pipeline to the data\npipelineModel = pipeline.fit(sampled_df)\n\n# Transform the data with the pipeline\nsampled_df_transformed = pipelineModel.transform(sampled_df)\n# Select features and target column for modeling\nsampled_df_model = sampled_df_transformed.select(\"features\", target_column)\n\n# Split data into training and test sets\ntrain_df, test_df = sampled_df_model.randomSplit([train_split, test_split])"}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(rf.numTrees, [10, 20, 30]) \\\n    .addGrid(rf.maxDepth, [5, 10]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ncross_val = CrossValidator(estimator=rf,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)\n\n# Make predictions on the test data using the best model\nbest_model = cv_model.bestModel\npredictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 0.9940029985007496\nBest Num Trees: 20, Best Max Depth: 10\n"}], "source": "# Evaluate the best model's performance\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Best Model Accuracy: {accuracy}\")\n\n# Optional: You can also view the best model's parameters\nbest_numTrees = best_model._java_obj.getNumTrees()\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nprint(f\"Best Num Trees: {best_numTrees}, Best Max Depth: {best_maxDepth}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# 4. Gradient Boosted Decision Trees"}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [], "source": "# Initialize Gradient Boosted Decision Trees Classifier\ngbt = GBTClassifier(labelCol=target_column, featuresCol=\"features\")\n\n# Train the model\ngbt_model = gbt.fit(train_df)"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 1.0\nTest Accuracy: 1.0\nArea Under ROC: 1.0\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = gbt_model.transform(train_df)\nevaluator_acc = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ntrain_accuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {train_accuracy}\")\n\n# Evaluate the model on the test set by accuracy\ntest_predictions = gbt_model.transform(test_df)\ntest_accuracy = evaluator_acc.evaluate(test_predictions)\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n# Calculate the area under the ROC curve (AUC)\nevaluator_auc = BinaryClassificationEvaluator(labelCol=target_column, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nauc = evaluator_auc.evaluate(test_predictions)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4.1 Hyperparameter Tuning"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [], "source": "# Define a grid of hyperparameters to search\nparam_grid = ParamGridBuilder() \\\n    .addGrid(gbt.maxDepth, [5, 10]) \\\n    .addGrid(gbt.maxIter, [10, 20]) \\\n    .build()\n\n# Set up the cross-validation\nevaluator = MulticlassClassificationEvaluator(labelCol=target_column, predictionCol=\"prediction\", metricName=\"accuracy\")\ncross_val = CrossValidator(estimator=gbt,\n                           estimatorParamMaps=param_grid,\n                           evaluator=evaluator,\n                           numFolds=5)\n\n# Run cross-validation to find the best hyperparameters\ncv_model = cross_val.fit(train_df)\n\n# Make predictions on the test data using the best model\nbest_model = cv_model.bestModel\npredictions = best_model.transform(test_df)"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 1.0\nBest Max Depth: 5, Best Max Iter: 10\n"}], "source": "# Evaluate the best model's performance\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Best Model Accuracy: {accuracy}\")\n\n# Optional: You can also view the best model's parameters\nbest_maxDepth = best_model._java_obj.getMaxDepth()\nbest_maxIter = best_model._java_obj.getMaxIter()\nprint(f\"Best Max Depth: {best_maxDepth}, Best Max Iter: {best_maxIter}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4.2 Feature importantces"}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Feature Importances:\ndeparture_airport_avg_altitude: 0.0\ndeparture_airport_avg_bearing: 3.3635742105411793e-18\ndeparture_airport_avg_knots: 0.0\ndeparture_airport_avg_temperature: 0.0\ndeparture_airport_min_altitude: 0.0\ndeparture_airport_min_bearing: 3.463430319916567e-17\ndeparture_airport_min_knots: 0.0\ndeparture_airport_min_temperature: 1.0695114872580156e-15\ndeparture_airport_max_altitude: 0.0\ndeparture_airport_max_bearing: 0.0\ndeparture_airport_max_knots: 0.0\ndeparture_airport_max_temperature: 8.072578105298831e-17\narrival_airport_avg_altitude: 0.0\narrival_airport_avg_bearing: 0.0\narrival_airport_avg_knots: 2.6406159786629843e-15\narrival_airport_avg_temperature: 0.0\narrival_airport_min_altitude: 0.0\narrival_airport_min_bearing: 3.602046366483853e-16\narrival_airport_min_knots: 0.0\narrival_airport_min_temperature: 7.995215898456383e-15\narrival_airport_max_altitude: 0.0\narrival_airport_max_bearing: 0.0\narrival_airport_max_knots: 5.789552109894005e-16\narrival_airport_max_temperature: 3.0944882736978846e-14\nflight_number: 0.0\nscheduled_air_time: 0.0\nscheduled_block_time: 0.0\nDelay: 0.9999999999998606\ndeparture_airport_code_bit_0: 0.0\ndeparture_airport_code_bit_1: 0.0\ndeparture_airport_code_bit_2: 0.0\ndeparture_airport_code_bit_3: 0.0\ndeparture_airport_code_bit_4: 3.673023037910968e-15\ndeparture_airport_code_bit_5: 0.0\ndeparture_airport_code_bit_6: 1.8583747513240053e-16\narrival_airport_code_bit_0: 2.860383508644219e-14\narrival_airport_code_bit_1: 0.0\narrival_airport_code_bit_2: 0.0\narrival_airport_code_bit_3: 0.0\narrival_airport_code_bit_4: 0.0\narrival_airport_code_bit_5: 0.0\narrival_airport_code_bit_6: 0.0\nairline_code_bit_0: 0.0\nairline_code_bit_1: 0.0\nairline_code_bit_2: 1.856692964218731e-15\nairline_code_bit_3: 5.129450671075296e-17\nairline_code_bit_4: 5.886254868447063e-16\nairline_code_bit_5: 0.0\nscheduled_aircraft_type_bit_0: 0.0\nscheduled_aircraft_type_bit_1: 6.390791000028242e-17\nscheduled_aircraft_type_bit_2: 0.0\nscheduled_aircraft_type_bit_3: 0.0\nscheduled_aircraft_type_bit_4: 3.715698385707209e-16\nscheduled_aircraft_type_bit_5: 0.0\nicao_aircraft_type_actual_bit_0: 0.0\nicao_aircraft_type_actual_bit_1: 0.0\nicao_aircraft_type_actual_bit_2: 0.0\nicao_aircraft_type_actual_bit_3: 0.0\nicao_aircraft_type_actual_bit_4: 0.0\nicao_aircraft_type_actual_bit_5: 7.367015858790758e-17\ndeparture_airport_bit_0: 0.0\ndeparture_airport_bit_1: 0.0\ndeparture_airport_bit_2: 0.0\ndeparture_airport_bit_3: 0.0\ndeparture_airport_bit_4: 0.0\ndeparture_airport_bit_5: 0.0\ndeparture_airport_bit_6: 9.619822242147772e-16\ndeparture_airport_bit_7: 1.2697492644792904e-16\ndeparture_airport_bit_8: 5.769318108783645e-17\narrival_airport_bit_0: 0.0\narrival_airport_bit_1: 0.0\narrival_airport_bit_2: 0.0\narrival_airport_bit_3: 0.0\narrival_airport_bit_4: 1.8499658157976486e-17\narrival_airport_bit_5: 2.3881376894842365e-16\narrival_airport_bit_6: 0.0\narrival_airport_bit_7: 7.694176006612875e-17\narrival_airport_bit_8: 0.0\naircraft_id_bit_0: 2.6235878842221193e-14\naircraft_id_bit_1: 0.0\naircraft_id_bit_2: 3.56538866317365e-16\naircraft_id_bit_3: 0.0\naircraft_id_bit_4: 0.0\naircraft_id_bit_5: 0.0\naircraft_id_bit_6: 1.4140465981115118e-14\naircraft_id_bit_7: 0.0\naircraft_id_bit_8: 1.997122187508825e-18\naircraft_id_bit_9: 0.0\naircraft_id_bit_10: 0.0\naircraft_id_bit_11: 0.0\naircraft_id_bit_12: 2.182959662641226e-15\nlegacy_route_bit_0: 0.0\nlegacy_route_bit_1: 0.0\nlegacy_route_bit_2: 0.0\nlegacy_route_bit_3: 0.0\nlegacy_route_bit_4: 0.0\nlegacy_route_bit_5: 1.1409243722155682e-14\nlegacy_route_bit_6: 0.0\nlegacy_route_bit_7: 0.0\nlegacy_route_bit_8: 0.0\nlegacy_route_bit_9: 0.0\nlegacy_route_bit_10: 0.0\nlegacy_route_bit_11: 0.0\nlegacy_route_bit_12: 0.0\nlegacy_route_bit_13: 0.0\nlegacy_route_bit_14: 0.0\nlegacy_route_bit_15: 0.0\nlegacy_route_bit_16: 0.0\nlegacy_route_bit_17: 1.0931616184258835e-16\nlegacy_route_bit_18: 4.318829286334875e-15\nbit_0: 0.0\nbit_1: 0.0\nbit_2: 0.0\nbit_3: 0.0\nImportant Features: ['Delay']\n"}], "source": "# Get feature importances\nfeature_importances = gbt_model.featureImportances\n\n# Print feature importances\nprint(\"Feature Importances:\")\nfor feature, importance in zip(feature_columns, feature_importances):\n    print(f\"{feature}: {importance}\")\n\n# Optionally, select the most important features based on a threshold\nthreshold = 0.001\nimportant_features = [feature for feature, importance in zip(feature_columns, feature_importances) if importance >= threshold]\nprint(\"Important Features:\", important_features)"}, {"cell_type": "markdown", "metadata": {}, "source": "The feature importance shows that it cannot get the important feature than label itself for Gradient Boosted Decision Trees model."}, {"cell_type": "markdown", "metadata": {}, "source": "# 5 Multilayer perceptron (MLP) "}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": "from pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\nlayers = [114, 64, 32, 2]\n\nmlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n\n"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": "train_df = train_df.withColumnRenamed(\"Delay\", \"label\")\ntest_df = test_df.withColumnRenamed(\"Delay\", \"label\")\n\nmodel = mlp.fit(train_df)\n"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set accuracy = 0.9554140127388535\n"}], "source": "result = model.transform(test_df)\n\n\npredictionAndLabels = result.select(\"prediction\", \"label\")\n\n\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\naccuracy = evaluator.evaluate(predictionAndLabels)\nprint(\"Test set accuracy = \" + str(accuracy))"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 0.5\n"}], "source": "# Calculate the area under the ROC curve (AUC)\nevaluator_auc = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nauc = evaluator_auc.evaluate(result)\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 5.1 Hyperparameter Tuning"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Number of features: 114\n"}], "source": "num_features = len(train_df.select(\"features\").first()[0])\nprint(f\"Number of features: {num_features}\")\n"}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best model parameters:\nfeaturesCol: features\nlabelCol: label\npredictionCol: prediction\nprobabilityCol: probability\nrawPredictionCol: rawPrediction\n"}], "source": "paramGrid = ParamGridBuilder() \\\n    .addGrid(mlp.maxIter, [100, 200]) \\\n    .addGrid(mlp.blockSize, [128, 256]) \\\n    .addGrid(mlp.layers, [[114, 5, 4, 2], [114, 6, 3, 2]]) \\\n    .build()\n\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\ncrossval = CrossValidator(estimator=mlp,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\ncvModel = crossval.fit(train_df)\n\nbestModel = cvModel.bestModel\nprint(\"Best model parameters:\")\n\nparams = bestModel.extractParamMap()\nfor param, value in params.items():\n    print(f\"{param.name}: {value}\")\n\n"}, {"cell_type": "code", "execution_count": 72, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Best model parameters and metric:\n{Param(parent='MultilayerPerceptronClassifier_05f02a343e92', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='MultilayerPerceptronClassifier_05f02a343e92', name='blockSize', doc='Block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. Recommended size is between 10 and 1000, default is 128.'): 128, Param(parent='MultilayerPerceptronClassifier_05f02a343e92', name='layers', doc='Sizes of layers from input layer to output layer E.g., Array(780, 100, 10) means 780 inputs, one hidden layer with 100 neurons and output layer of 10 neurons.'): [114, 5, 4, 2]}\nBest metric (accuracy): 0.9609760183726678\n"}], "source": "params_and_metrics = list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))\n\nbest_params = params_and_metrics[0][0]\nbest_metric = params_and_metrics[0][1]\nfor params, metric in params_and_metrics:\n    if metric > best_metric:\n        best_params = params\n        best_metric = metric\n\nprint(\"Best model parameters and metric:\")\nprint(best_params)\nprint(f\"Best metric (accuracy): {best_metric}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# 6 Linear Support Vector Machine"}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": "from pyspark.ml.classification import LinearSVC\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import VectorAssembler"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n\nlsvcModel = lsvc.fit(train_df)"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set accuracy = 1.0\n"}], "source": "predictions = lsvcModel.transform(test_df)\n\nevaluator = BinaryClassificationEvaluator()\naccuracy = evaluator.evaluate(predictions)\n\nprint(\"Test set accuracy = \" + str(accuracy))\n"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training Accuracy: 1.0\n"}], "source": "# Evaluate the model on the training set by accuracy\ntrain_predictions = lsvcModel.transform(train_df)\n# by accuracy\nevaluator_acc = BinaryClassificationEvaluator()\naccuracy = evaluator_acc.evaluate(train_predictions)\nprint(f\"Training Accuracy: {accuracy}\")"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Area Under ROC: 1.0\n"}], "source": "# Calculate the area under the ROC curve (AUC)\nauc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\nprint(f\"Area Under ROC: {auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6.1 Hyperparameter Tuning"}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [], "source": "paramGrid = ParamGridBuilder() \\\n    .addGrid(lsvc.maxIter, [10, 100, 1000]) \\\n    .addGrid(lsvc.regParam, [0.01, 0.1, 0.5]) \\\n    .build()\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set AUC after hyperparameter tuning = 1.0\n"}], "source": "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n\ncrossval = CrossValidator(estimator=lsvc,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3) \n\ncvModel = crossval.fit(train_df)\n\nbestModel = cvModel.bestModel\npredictions = bestModel.transform(test_df)\n\ntest_auc = evaluator.evaluate(predictions)\nprint(f\"Test set AUC after hyperparameter tuning = {test_auc}\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 'LinearSVCModel' has no 'featureImportances'"}, {"cell_type": "markdown", "metadata": {}, "source": "# 7 OneVsRes"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [], "source": "from pyspark.ml.classification import OneVsRest, LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": "classifier = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n\novr = OneVsRest(classifier=classifier)\n\novrModel = ovr.fit(train_df)\n"}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set accuracy = 1.0\n"}], "source": "predictions = ovrModel.transform(test_df)\n\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(f\"Test set accuracy = {accuracy}\")\n"}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Training set accuracy = 1.0\n"}], "source": "train_predictions = ovrModel.transform(train_df)\n\ntrain_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\ntrain_accuracy = train_evaluator.evaluate(train_predictions)\n\nprint(f\"Training set accuracy = {train_accuracy}\")\n"}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set AUC = 1.0\n"}], "source": "auc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\ntest_auc = auc_evaluator.evaluate(predictions)\n\nprint(f\"Test set AUC = {test_auc}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "##  7.1 Hyperparameter Tuning"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [], "source": "paramGrid = ParamGridBuilder() \\\n    .addGrid(classifier.maxIter, [10, 50]) \\\n    .addGrid(classifier.regParam, [0.01, 0.1]) \\\n    .build()\n"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set accuracy after hyperparameter tuning = 1.0\n"}], "source": "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n\ncrossval = CrossValidator(estimator=ovr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3) \n\ncvModel = crossval.fit(train_df)\n\nbestModel = cvModel.bestModel\npredictions = bestModel.transform(test_df)\n\ntest_accuracy = evaluator.evaluate(predictions)\nprint(f\"Test set accuracy after hyperparameter tuning = {test_accuracy}\")\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 5}